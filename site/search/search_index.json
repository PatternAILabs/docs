{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"PatternAI Agent Lab <p>Streaming solutions for rising needs.</p> Get Started <p>exit</p>"},{"location":"agent-build/","title":"PatternAI Agent: Voicing a Change","text":"<p>In these times, where attention is fleeting and retention is hard-won, people value speed, ease, and familiarity more than ever. User convenience is king and businesses must connect with people on the platforms they already use. PatternAI Agent meets users where they are, transforming everyday chat apps into intelligent, voice-powered assistants that feel natural and effortless. Whether it's placing an order, getting support, or booking a trip, customers can simply send a voice message or a text, and get instant, helpful responses.</p> <p>PatternAI Agent lets people talk naturally with a smart assistant on WhatsApp \u2014 using voice or text \u2014 just like chatting with a friend.</p> <p>Built with the flexibility of developers and the needs of enterprises in mind, PatternAI integrates cutting-edge AI models for speech, language, and dialogue \u2014 all within a WhatsApp-native experience.</p>"},{"location":"agent-build/#what-makes-it-great","title":"What Makes It Great?","text":"<p>Can't type? Talk: Customers can speak naturally to a WhatsApp assistant, hands-free or type personifying it into a companion.</p> <p>Familiar &amp; Convenient: No new apps to download. Everything happens right inside WhatsApp, a platform people already love.</p> <p>Instant Replies: Get instant, streaming responses for fluid conversations.</p> <p>Always Reliable: Even in noisy or difficult situations, it keeps conversations smooth and helpful.</p>"},{"location":"agent-build/#whos-it-for","title":"Who's It For?","text":""},{"location":"agent-build/#1-businesses-clients-developers","title":"1. Businesses / Clients / Developers:","text":"<p>These are the people or companies who adopt and integrate PatternAI Agent into their services.</p> <p>Examples: - E-commerce platforms that want voice-based customer support. - Travel agencies offering real-time, conversational bookings via WhatsApp. - Customer support teams looking to reduce manual workloads. - Developers / system integrators building custom agents for clients.</p>"},{"location":"agent-build/#2-end-users-consumers-customers","title":"2. End Users (Consumers / Customers):","text":"<p>These are the users of WhatsApp who interact with the PatternAI-powered agents without knowing what's behind the scenes.</p> <p>Examples: - A customer tracking their online order via voice message. - Someone booking a trip through WhatsApp by speaking to an assistant. - Someone seeking product recommendations in a native language.</p>"},{"location":"agent-build/#lets-converse-step-by-step","title":"Let's Converse, Step by Step","text":"<p>With PatternAI, conversations on WhatsApp just flow \u2014 smart, fast, and surprisingly human.</p> <p></p> <p>When you send a voice message:</p> <ol> <li> <p>Your message is heard.    The assistant listens and understands what you said.</p> </li> <li> <p>It figures out what you need.    Whether it's booking a flight, checking an order, or answering a question \u2013 it gets the context.</p> </li> <li> <p>You get a quick, helpful reply.    The assistant responds instantly in voice \u2013 based on what works best for you.</p> </li> </ol> <p>That's it. No waiting. No typing (unless you want to). Just natural conversations.</p>"},{"location":"agent-build/#frequently-asked-questions","title":"\u2753 Frequently Asked Questions","text":""},{"location":"agent-build/#1-can-i-change-the-voice-of-the-assistant","title":"1. Can I change the voice of the assistant?","text":"<p>Not at the moment. The assistant uses a fixed voice that's chosen by the developers for clarity and consistency. While the system supports different voice types internally, users cannot change the voice directly. We may offer more voice options in future updates.</p>"},{"location":"agent-build/#2-how-do-i-clear-all-my-data-from-the-assistant","title":"2. How do I clear all my data from the assistant?","text":""},{"location":"agent-build/#3-can-i-book-an-uber-using-the-agent","title":"3. Can I book an Uber using the agent?","text":"<p>Not yet \u2014 but integrations like Uber are in our roadmap. Stay tuned for updates!</p>"},{"location":"agent-build/#4-is-my-personal-data-shared-between-domains-or-stored-without-my-consent","title":"4. Is my personal data shared between domains or stored without my consent?","text":"<p>No. PatternAI is privacy-first. Your data is only used for that specific interaction unless you explicitly allow otherwise.</p>"},{"location":"agent-build/#5-how-much-data-is-saved-to-the-system","title":"5. How much data is saved to the system?","text":"<p>Only the data needed to process your current request is temporarily stored. For transparency, the assistant uses Langfuse to track performance and cost, not to store personal content permanently.</p>"},{"location":"agent-build/#6-can-i-send-a-photo-or-video-of-the-items-i-want-to-order","title":"6. Can I send a photo or video of the items I want to order?","text":"<p>For now, only voice and text messages are supported. Visual input features may be added in future versions.</p>"},{"location":"agent-build/#7-can-i-list-places-i-want-to-visit-and-have-the-agent-plan-a-route","title":"7. Can I list places I want to visit and have the agent plan a route?","text":""},{"location":"agent-build/#8-what-if-my-voice-message-had-a-mistake-can-i-correct-it","title":"8. What if my voice message had a mistake? Can I correct it?","text":"<p>No worries! You can simply say \"correct that\" or resend the message \u2014 the assistant will wait for your confirmation before moving on.</p>"},{"location":"agent-build/#9-do-customers-need-to-install-anything","title":"9. Do customers need to install anything?","text":"<p>No installation required \u2014 everything runs inside WhatsApp. Just send a message and you're in.</p>"},{"location":"agent-build/#10-how-many-languages-can-the-assistant-understand","title":"10. How many languages can the assistant understand?","text":"<p>The assistant can recognize multiple languages depending on the speech model (like Google STT or Whisper). It automatically detects the language based on your voice or text input.</p>"},{"location":"agent-build/#whats-next","title":"What's Next?","text":"<p>We are currently in the process of integrating more advanced features and domains, more updates on the way\u2026</p>"},{"location":"agent-build/#try-it-for-yourself","title":"Try it for yourself!","text":"<p>PatternAI Agent is here to make your daily tasks easier \u2014 your assistant listens, understands, and responds \u2014 just like a real conversation.</p> <p>Ready to START?</p> <p>\"Say it. Solve it.\"</p> <p></p>"},{"location":"agent-build/developer/","title":"Developer Insight into PatternAI Agent","text":""},{"location":"agent-build/developer/#overview","title":"Overview","text":"<p>PatternAI Agent is a modular, voice-enabled conversational agent framework built to deliver seamless, human-like interactions via audio and text channels through WhatsApp.</p> <p></p> <p>The system integrates multiple Speech-to-Text (STT) and Text-to-Speech (TTS) services, connects to powerful Large Language Models (LLMs) through OpenRouter, and leverages Langfuse for prompt tracing and monitoring. It's designed with a clean backend and modular agent profiles for domain-specific use cases like shopping, travel assistance, etc.</p>"},{"location":"agent-build/developer/#project-walkthrough-diving-into-the-system","title":"Project Walkthrough: Diving into the System","text":"<p>To understand the operation of the PatternAI Agent, this section will guide through the process flow, from the initial receipt of a user's request to the generation and delivery of a response. Key system components and their interactions will be unpacked.</p>"},{"location":"agent-build/developer/#the-foundation","title":"The Foundation","text":"<ul> <li>/app : This directory houses the core application logic and services that drive the agent's functionality. It includes:</li> </ul> <p>/services : encapsulates the core AI processing logic of the PatternAI Agent. It defines abstract interfaces and provides concrete implementations for key AI processing capabilities.</p> <p>These capabilities are organized into the following sub-modules:</p> services/ <pre><code>services/\n\u251c\u2500\u2500 llm/\n\u2502   \u251c\u2500\u2500 __init__.py                # Factory: creates LLM handler (OpenRouter/custom)\n\u2502   \u251c\u2500\u2500 base_handler.py            # Abstract base class for LLMs (defines prompt + interface)\n\u2502   \u251c\u2500\u2500 openrouter_handler.py      # Implementation using OpenRouter API\n\u2502   \u251c\u2500\u2500 router_handler.py          # Routes queries to tools based on user intent\n\u2502   \u251c\u2500\u2500 conversation_history.py    # Maintains chat history with truncation logic\n\u2502   \u251c\u2500\u2500 prompts/                   # Prompt management logic (Langfuse or static)\n\u2502   \u251c\u2500\u2500 tools/                     # Tool execution logic and OpenAI-compatible tool specs\n\u2502   \u2514\u2500\u2500 validators/                # Streaming and final response validation logic\n\u2502\n\u251c\u2500\u2500 stt/\n\u2502   \u251c\u2500\u2500 __init__.py                # Factory: selects STT engine\n\u2502   \u251c\u2500\u2500 base_stt_handler.py        # Abstract base class for STT services\n\u2502   \u251c\u2500\u2500 whisper_stt.py             # STT via OpenAI Whisper API\n\u2502   \u251c\u2500\u2500 google_stt.py              # STT via Google Cloud Speech API\n\u2502   \u251c\u2500\u2500 elevenlabs_stt.py          # STT via ElevenLabs API\n\u2502   \u2514\u2500\u2500 stt_validation.py          # Validation and metrics for STT transcription\n\u2502\n\u2514\u2500\u2500 tts/\n    \u251c\u2500\u2500 __init__.py               # Factory: selects TTS engine\n    \u251c\u2500\u2500 base_tts_handler.py       # Abstract TTS base class, defines text-to-audio interface\n    \u251c\u2500\u2500 edge_tts.py               # TTS implementation using Microsoft Edge TTS\n    \u2514\u2500\u2500 tts_validation.py         # Validation for text-to-speech output \n</code></pre> <p>/llm : orchestrates intelligent conversation flow through a sophisticated multi-layered architecture that transforms user intent into contextual, validated responses.Rather than simply connecting to language models, this module implements a complete reasoning pipeline that routes requests, maintains conversation context, validates outputs, and ensures reliable agent behavior.</p>"},{"location":"agent-build/developer/#llm-service-multi-agent-architecture","title":"LLM Service: Multi-Agent Architecture","text":""},{"location":"agent-build/developer/#tier-1-intent-recognition-tool-selection","title":"Tier 1 : Intent Recognition &amp; Tool Selection","text":"<p>How does the system know where to send a request like \"I need donuts\"?</p> <p>It\u2019s handled by <code>router_handler.py/</code> and <code>tools.py/</code> using a dedicated LLM as an intelligent dispatcher. Using the configured LLM, it analyzes user intent and selects the most appropriate specialized tool from the available arsenal defined in tools_config.yaml. This isn't simple keyword matching - the router understands context, recognizing the difference between \"I need coffee\" (shopping intent) and \"I want to travel to coffee county\" (travel intent). The routing system leverages a structured approach:</p> <ul> <li>Analyzes user input against tool descriptions from ROUTER_SYSTEM_PROMPT</li> <li>returns JSON-formatted routing decisions via <code>RouterHandler.route()</code></li> <li><code>initialize_tools()</code> creates bidirectional mappings between tools and agents</li> <li>Tools operate through unified interface <code>base_tool.py</code> with consistent behavior</li> <li>Each tool knows its agent ID, enabling seamless transition to specialized processing</li> </ul> <p>Note</p> <p>New domains can be added by simply creating new tool classes and updating the YAML configuration</p>"},{"location":"agent-build/developer/#tier-2-contextual-conversation-management","title":"Tier 2 : Contextual Conversation Management","text":"<p>Before any response generation begins, the layer <code>conversation_history.py/</code> enriches the interaction with memory. The ConversationHistory class maintains an intelligent sliding window of past interactions, applying both message count limits and token-based truncation to optimize context while preserving memory efficiency.</p>"},{"location":"agent-build/developer/#tier-3-response-generation-built-in-validation","title":"Tier 3 : Response Generation &amp; Built-in Validation","text":"<p>This layer orchestrates LLM interaction through OpenRouterHandler, serving as the bridge between the internal agent system and external language models. This handler manages the complete response lifecycle: This handler manages the complete response lifecycle with integrated prompt management, streaming generation, real-time Pydantic validation of all streaming LLM responses to ensure structured output formatting and type safety.</p> <p>/stt : a unified interface for converting speech to text using multiple cloud providers. It supports Whisper (OpenAI), Google Cloud Speech-to-Text, and ElevenLabs services through a common abstraction layer.</p> <p>/tts : a standardized interface for converting text to speech using various synthesis engines. Currently implemented with Microsoft Edge's TTS service, the module is designed for easy extensibility to support additional providers.</p>"},{"location":"agent-build/developer/#service-traversal","title":"Service Traversal","text":"<p>The system operates through a series of coordinated steps that leverage the abstract interfaces and factory pattern to process user requests. </p> <p>When a client interacts with the system, the request follows a consistent architectural pattern across all service types (STT, LLM, and TTS):</p> <p>While processing an audio file, the system uses factory functions like <code>create_stt_service</code> to instantiate the appropriate service (Whisper, Google STT, or ElevenLabs). These factories handle configuration parsing, environment variable injection, and service initialization in one centralized location.</p> <p>After initialization, the system interacts with each service through its abstract interface. For example, calling <code>transcribe()</code> on any STT service produces a text transcript regardless of which underlying provider (Whisper, Google, or ElevenLabs) is actually processing the audio.</p> <p>The LLM processing and text-to-speech (TTS) flows follow a similar architectural pattern.</p> <ul> <li>For LLMs, the system uses create_llm_handler to instantiate an OpenRouterHandler, which acts as the gateway to OpenRouter-backed language models. It supports prompt injection, conversation context, real-time streaming, and structured response validation, as detailed in the LLM service section above.</li> <li>For TTS, the <code>create_tts_service</code> factory function initializes an EdgeTTS service that converts text input into audio bytes using the Microsoft Edge TTS engine.</li> </ul> <p>While models provide services by default, their performance can vary, a topic we'll explore using Langfuse later. Keeping that in mind, we've built a framework that allows developers to integrate models as they please.</p>"},{"location":"agent-build/developer/#how-to-plug-in","title":"How to plug in:","text":"<p>For instance, let's integrate a new STT service:</p> <ul> <li>Implement a handler class inheriting from <code>BaseSTTClass</code>.</li> <li>Add it to the <code>service_registry</code> in <code>create_stt_service</code>.</li> </ul> <pre><code>service_registry = {\n    \"whisper\": WhisperSTT,\n    \"google\": GoogleSTT,\n    \"elevenlabs\": ElevenLabsSTT,\n    \"new_service\": NewSTTService  # add here\n}\n</code></pre> <p>Also we've optimized performance by implementing factory functions with LRU caching and custom cache logging decorators, preventing redundant service creation when the same configuration is requested multiple times. This allows dynamic, efficient, and configurable creation of service instances at runtime.</p> <p>/tests : This directory contains unit and integration tests for the application logic, ensuring the reliability and correctness of the core functionalities. Tests are executed using the pytest framework.</p> <p>To run all tests, use the following command:</p> <pre><code>poetry run pytest\n</code></pre>"},{"location":"agent-build/developer/#test-structure","title":"Test Structure","text":"<p>The /tests directory is organized by module, with each file typically containing tests for a specific part of the application:</p> <ul> <li><code>test_llm.py</code>: Contains tests for the Large Language Model (LLM) integration, including schema validation and semantic comparison of LLM outputs</li> <li><code>test_stt.py</code>: Contains tests for the Speech-to-Text (STT) service, covering different STT providers and transcription functionality</li> <li><code>test_tts.py</code>: Contains tests for the Text-to-Speech (TTS) service, including service creation, configuration, and speech synthesis</li> <li><code>conftest.py</code>: Provides helper functions and fixtures used across multiple test files. For example, the <code>cleaned_json</code> function normalizes JSON outputs from LLMs, which is crucial for testing LLM responses</li> </ul>"},{"location":"agent-build/developer/#refine-tune","title":"Refine &amp; Tune","text":"<p>Creating safer, more humane conversational agents meant taking on the responsibility of ensuring responses remain fair, respectful, and trustworthy \u2014 not just intelligent.</p> <p>With that in mind, /filters : Provides a modular text filtering framework that enforces ethical and contextual guardrails for agent outputs. It is responsible for detecting and handling:  </p> <ul> <li>Personally Identifiable Information (PII)  </li> <li>Biased language  </li> <li>Profanity  </li> </ul> <p>Every filter is pluggable and configurable, allowing easy customization per agent use case.</p>"},{"location":"agent-build/developer/#design-philosophy","title":"Design Philosophy","text":"<p>Filters follow the Strategy Pattern, built on a shared <code>BaseFilter</code> interface. Each filter implements an <code>apply(text: str) -&gt; str</code> method, ensuring consistency and interchangeability.</p> <pre><code>class BaseFilter(ABC):\n    @abstractmethod\n    def apply(self, text: str) -&gt; str:\n        \u2026\n</code></pre> <p>Note</p> <p>This design allows us to dynamically register and apply filters, configured declaratively through a YAML file.</p>"},{"location":"agent-build/developer/#rule-engine","title":"Rule Engine","text":"<p>At runtime, the RuleEngine loads filters defined in configs.yaml, instantiates them, and applies them sequentially to incoming text enabling  centralized control,  easy reordering or disabling of filters and detailed logging of filter application.</p>"},{"location":"agent-build/developer/#filters-in-use","title":"Filters in Use","text":"<ul> <li>BiasFilter \u2013 Replaces predefined biased terms with neutral ones using regex-based replacements.</li> <li>PIIFilter \u2013 Detects and anonymizes PII in plain text and json using presidio, with field-specific handling for dates and locations.</li> <li>ProfanityFilter \u2013 Censors offensive language using both regex and the better_profanity library.</li> </ul>"},{"location":"agent-build/developer/#versatility-and-modularity","title":"Versatility and Modularity","text":"<ul> <li>/clients : provides a flexible framework for implementing domain-specific conversational agents with WhatsApp integration. Each client gets its own isolated configuration and implementation while sharing core AI services (STT/TTS/LLM).</li> </ul>"},{"location":"agent-build/developer/#api-endpoints","title":"API Endpoints","text":"<p>With the core services for STT, TTS, and LLM dynamically monitored through our modular framework, these capabilities are made accessible to client applications via a set of clean, RESTful API endpoints. These endpoints handle everything from speech transcription to language and audio generation, providing a unified interface for seamless multi-modal conversations.</p> <p>The service exposes the following endpoints:</p> Endpoint Method Description <code>/health</code> GET Health check endpoint <code>/stt</code> POST Speech-to-Text transcription <code>/llm</code> POST Process text with LLM models <code>/tts</code> POST Text-to-Speech synthesis"},{"location":"agent-build/developer/#health-check","title":"Health Check","text":"<p>Verify if the service is running:</p> <pre><code>curl -X GET \"http://localhost:5000/health\"\n</code></pre> <p>Response: <pre><code>{\"status\": \"ok\"}\n</code></pre></p>"},{"location":"agent-build/developer/#1-speech-to-text-stt","title":"1. Speech-to-Text (STT)","text":"<p>Convert audio to text using the <code>/stt</code> endpoint.</p> <p>Request</p> <pre><code>curl -X POST \"http://localhost:5000/stt\" \\\n  -H \"Content-Type: application/octet-stream\" \\\n  -H \"STT-Model: whisper\" \\\n  --data-binary @\"audio_file.webm\"\n</code></pre> <p>Available STT models: - <code>whisper</code> (OpenAI's Whisper model) - <code>google</code> (Google Speech-to-Text) - <code>elevenlabs</code> (ElevenLabs STT service)</p> <p>The model can be selected via the <code>STT-Model</code> header. If not specified, the default model configured in <code>configs.yaml</code> will be used.</p> <p>Response: <pre><code>{\n  \"status\": \"success\",\n  \"transcription\": \"This is the transcribed text from the audio.\",\n  \"request_id\": \"unique-request-id\"\n}\n</code></pre></p>"},{"location":"agent-build/developer/#2-language-model-llm-processing-sub-heading","title":"2. Language Model (LLM) Processing{: .sub-heading}","text":"<p>Process transcribed text with an LLM model using the <code>/llm</code> endpoint.</p> <p>Request</p> <pre><code>curl -X POST \"http://localhost:5000/llm\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"transcription\": \"What items do you have on sale?\",\n    \"model_name\": \"anthropic/claude-3-haiku-20240307\",\n    \"agent_id\": \"agent_shopping\"\n  }'\n</code></pre> <p>Notes: - <code>model_name</code>: Specifies which LLM model to use (required if <code>fallback_enabled</code> is set to false in configuration) - <code>agent_id</code>: Optional parameter to specify which agent profile to use (e.g., \"agent_shopping\", \"agent_travel\")</p> <p>Response:</p> <p>The response is streamed in plain text format:</p> <pre><code>request_id: unique-request-id\n\nWe have several items on sale this week. Our fresh produce section has discounts on seasonal fruits and vegetables. In the grocery aisle, we have deals on pasta, rice, and canned goods. The bakery section has a buy-one-get-one offer on whole wheat bread. Is there anything specific you're looking for?\n</code></pre>"},{"location":"agent-build/developer/#3-text-to-speech-tts","title":"3. Text-to-Speech (TTS)","text":"<p>Convert text to speech using the <code>/tts</code> endpoint.</p> <p>Request</p> <pre><code>curl -X POST \"http://localhost:5000/tts\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"TTS-Model: edge_tts\" \\\n  -d '{\n    \"message\": \"Hello, we have several items on sale this week. Is there anything specific you'\\''re looking for?\"\n  }' \\\n  --output speech.mp3\n</code></pre> <p>Available TTS models: - <code>edge_tts</code> (Microsoft Edge TTS)</p> <p>Response:</p> <p>The response is an audio file in MP3 format with appropriate headers:</p> <pre><code>Content-Type: audio/mpeg\nContent-Disposition: attachment; filename=speech.mp3\nX-Request-ID: unique-request-id\n</code></pre>"},{"location":"agent-build/developer/#langfuse-for-prompt-management","title":"Langfuse For Prompt Management","text":"<p>\"For a transparent view into the agent's language-driven interactions and a pathway to iterative improvement, the framework leverages Langfuse for robust prompt management and comprehensive monitoring.\"</p> <p>To ensure that PatternAI Agent remains highly modular and adaptable, prompt management is architected with a clear separation of concerns.</p> <p>We've established an abstract Base Interface (<code>BasePromptManager</code>) - defined in <code>/base_prompt_handler.py</code>, which serves as an abstract interface for prompt retrieval. This allows us to seamlessly integrate various prompt management solutions without impacting the core logic that interacts with LLMs.</p> <p>Our current implementation LangfusePromptManager - defined in <code>/langfuse_handler.py</code>, leverages the Langfuse SDK to dynamically fetch prompt definitions from a centralized prompt store. This approach offers several key advantages:</p> <ul> <li>Centralized Prompt Management: All prompt templates are managed within Langfuse, providing a single source of truth and simplifying updates.</li> <li>Dynamic Updates: Changes to prompts in Langfuse are reflected in the agent without requiring code deployments.</li> <li>Version Control &amp; History: Langfuse provides built-in versioning and history tracking for prompts, crucial for experimentation and rollback.</li> <li>Observability: Langfuse integrates seamlessly with prompt tracing and monitoring, offering valuable insights into prompt performance and usage.</li> </ul> <p></p> <p></p> <p></p> <p>The <code>PromptManager</code> interface dictates that any concrete implementation must provide a <code>get_prompt(name)</code> method. Our <code>LangfusePromptManager</code> fulfills this contract by initializing the Langfuse SDK using environment variables (<code>LANGFUSE_PUBLIC_KEY</code>, <code>LANGFUSE_SECRET_KEY</code>, <code>LANGFUSE_HOST</code>) and retrieving prompts by their unique name using the Langfuse client:</p> <pre><code>self.client.get_prompt(name)\n</code></pre> <p>This decoupling empowers us to evolve our prompt management strategy in the future \u2013 simply by implementing a new class that adheres to the <code>PromptManager</code> interface. This design ensures that the PatternAI Agent remains flexible and maintainable as our requirements evolve.</p> <p>Note</p> <p>Want to customize or extend the monitoring?  You can easily plug in your own prompt management or analytics solution too.  To learn more about Langfuse, visit their official documentation (https://langfuse.com/docs).</p>"},{"location":"agent-build/developer/#langfuse-for-tracking","title":"Langfuse For Tracking","text":"<p>To gain precise observability into the performance and behavior of our multi-modal services \u2013 including language modeling (LLM), speech-to-text (STT), and text-to-speech (TTS) \u2013 we integrate Langfuse. This provides structured, end-to-end tracing of interactions across services, enabling detailed logging of input/output data, metadata, cost, and latency for each API call in our application pipeline.</p>"},{"location":"agent-build/developer/#why-we-use-it-here","title":"Why We Use It Here","text":"<p>In multi-modal AI architectures, especially those combining LLM, STT, and TTS services, monitoring individual service calls and aggregating their performance and cost metrics can be complex. Langfuse simplifies this by:</p> <ul> <li>Enabling real-time cost tracking per API call.</li> <li>Capturing performance metrics such as latency, token usage, audio duration, and processing time.</li> <li>Providing unified tracing across services, allowing correlation of input/output pairs and their associated metadata.</li> <li>Supporting prompt versioning and retrieval analytics for LLM generations.</li> </ul>"},{"location":"agent-build/developer/#langfuse-integration-usage","title":"Langfuse Integration &amp; Usage","text":"<p>To enable request tracing and detailed observability across all AI components, Langfuse is initialized using credentials securely stored in environment variables.</p> <p>Langfuse's SDK is initialized in each tracker with:</p> <pre><code>self.langfuse = Langfuse(\n    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n    host=os.getenv(\"LANGFUSE_HOST\")\n)\n</code></pre> <p>This Langfuse instance is then used throughout the application to track operations and metrics for each service.</p>"},{"location":"agent-build/developer/#example-usage","title":"Example Usage","text":"<p>Langfuse tracing relies primarily on two methods:</p> <ul> <li><code>trace()</code> is used to start a new tracing session, encapsulating the full flow of a request.</li> <li><code>generation()</code> records specific input/output events, such as transcriptions, responses, or synthesized speech, including performance and cost metadata.</li> </ul> <p>Example: Starting a trace</p> <pre><code>trace = self.langfuse.trace(\n    name=\"cost_analysis\",\n    user_id=user_id,\n    metadata={\"environment\": os.getenv(\"ENV\", \"dev\"), \"source\": \"speech_input\"}\n)\n</code></pre> <p>Example: Logging a generation event</p> <pre><code>trace.generation(\n    name=f\"stt-transcription-{model_name}\",\n    input=f\"[Audio duration: {audio_duration_sec:.2f}s]\",\n    output=transcription,\n    model=model_name,\n    metadata={\n        \"duration_audio\": round(audio_duration_sec, 8),\n        \"cost\": round(cost, 8),\n        \"currency\": \"USD\",\n    }\n)\n</code></pre>"},{"location":"agent-build/developer/#service-specific-trackers","title":"Service specific trackers","text":"<p>We encapsulate Langfuse functionality within dedicated tracker classes for each core service.</p> <p>LLMTracker \u2192 <code>/clients/demo/llm_tracker.py</code></p> <p>Handles all tracing related to LLM interactions. It initializes with model-specific configuration such as token pricing and supported providers from <code>configs.yaml</code>, starts a trace with <code>start_trace()</code>, and records generation event with Langfuse using <code>track_llm()</code>.</p>"},{"location":"agent-build/developer/#metrics-tracked","title":"Metrics Tracked:","text":"<ul> <li>Input/output tokens</li> <li>Latency and response time</li> <li>Cost per model/token type</li> <li>Prompt metadata</li> </ul> <p>STTTracker \u2192 <code>/clients/demo/stt_tracker.py</code></p> <p>Manages speech-to-text tracing. It loads model and pricing info, starts a trace via <code>start_trace()</code>, and logs the transcription with <code>track_stt()</code>.</p>"},{"location":"agent-build/developer/#metrics-tracked_1","title":"Metrics Tracked:","text":"<ul> <li>Audio duration</li> <li>Transcription text</li> <li>STT model used</li> <li>Cost per second</li> </ul> <p>TTSTracker \u2192 <code>/clients/demo/tts_tracker.py</code></p> <p>Tracks text-to-speech operations. At initialization, it reads the default voice model and pricing scheme\u2014covering both per-character and per-second billing\u2014from the config, starts a trace with <code>start_trace()</code>, and logs synthesis details in <code>track_tts()</code>.</p>"},{"location":"agent-build/developer/#metrics-tracked_2","title":"Metrics Tracked:","text":"<ul> <li>Character count</li> <li>Audio duration</li> <li>Processing time</li> <li>Model and voice used</li> <li>Combined cost (char + audio-based)</li> </ul> <p>All configuration values \u2014 such as pricing, model names, and default services \u2014 are centrally managed via <code>configs.yaml</code>.</p> <p>After each conversation, the metrics are calculated and visualized in Langfuse's dashboard, where teams monitor cost-per-session or drill into LLM token usage by model. </p> <p></p>"},{"location":"agent-build/developer/#whatsapp-integration","title":"WhatsApp Integration","text":"<p>Support for WhatsApp messaging is currently being developed to enable seamless voice and text interactions through the platform.  Hop into Whatsapp Integration.</p> <p>This document has provided a comprehensive overview of the PatternAI Agent, a modular framework for building voice-enabled conversational agents. We covered the system's architecture, key components, and how they interact to process user requests. The agent's flexibility is enabled by its modular design, support for multiple STT, TTS, and LLM providers, and the use of Langfuse for detailed observability.</p>"},{"location":"agent-build/developer/#next-steps","title":"Next Steps","text":"<p>To get started with the PatternAI Agent:</p> <ol> <li>Ensure you have met all the system requirements.</li> <li>Follow the instructions in the Installation Guide to set up the environment and dependencies.</li> <li>Explore the example clients in the <code>clients/</code> directory.</li> </ol>"},{"location":"agent-build/developer/#resources","title":"Resources","text":"<ul> <li>GitHub Repository: PatternAI Agent</li> </ul> <p>We encourage you to leverage the PatternAI Agent to create innovative and engaging conversational experiences. We welcome your feedback and contributions!</p> <p></p>"},{"location":"agent-build/prerequisites/","title":"Prerequisites","text":""},{"location":"agent-build/prerequisites/#prerequisites","title":"Prerequisites","text":"<p>Before installing and running the application, ensure the following:</p> <ul> <li>Python 3.12+ - (Required due to version-specific features)</li> <li>Poetry - (For dependency and environment management)</li> <li>API Keys for external services:<ul> <li>OpenAI (for Whisper STT)</li> <li>Google Cloud (for Google STT)</li> <li>ElevenLabs (for ElevenLabs STT)</li> <li>OpenRouter (for LLM access)</li> <li>Langfuse (for prompt management and tracing)</li> </ul> </li> </ul>"},{"location":"agent-build/projects/","title":"Projects","text":""},{"location":"agent-build/setup/","title":"Installation Guide","text":"<p>Pattern AI agent is a versatile, voice enabled agent designed to enhance and streamline how users converse with digital services across multiple domains. These interactions remain human-like, with tightly tailored responses that allow the agent to serve as a true partner in meeting users needs and requests.</p>"},{"location":"agent-build/setup/#installation-guide","title":"Installation Guide","text":"<p>Follow these steps to set up and run the project using Poetry.</p>"},{"location":"agent-build/setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/PatternAILabs/patternai-agent-build.git\ncd patternai-agent-build\n</code></pre>"},{"location":"agent-build/setup/#2-install-poetry-if-not-already-installed","title":"2. Install Poetry (if not already installed)","text":"<p>For macOS/Linux: <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre></p> <p>For Windows (using PowerShell): <pre><code>(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\n</code></pre></p>"},{"location":"agent-build/setup/#3-configure-poetry-for-in-project-virtual-environments","title":"3. Configure Poetry for In-Project Virtual Environments","text":"<pre><code>poetry config virtualenvs.in-project true\n</code></pre>"},{"location":"agent-build/setup/#4-initialize-and-install-dependencies","title":"4. Initialize and Install Dependencies","text":"<pre><code>poetry install\n</code></pre>"},{"location":"agent-build/setup/#5-activate-the-virtual-environment","title":"5. Activate the Virtual Environment","text":"<p>For macOS/Linux: <pre><code>source $(poetry env info --path)/bin/activate\n</code></pre></p> <p>For Windows: <pre><code>.venv\\Scripts\\activate\n</code></pre></p>"},{"location":"agent-build/setup/#6-set-up-environment-variables","title":"6. Set Up Environment Variables","text":"<p>Create a <code>.env</code> file in the project root with the following variables:</p> <pre><code># AI Service APIs\nOPENROUTER_API_KEY=your_openrouter_key\nOPENAI_API_KEY=your_openai_key_here\nELEVENLABS_API_KEY=your_elevenlabs_key\n\n# Google Cloud Credentials (JSON format)\nGOOGLE_APPLICATION_CREDENTIALS={\"type\": \"service_account\", \"project_id\": \"your-project-id\", ...}\n\n# Langfuse Configuration\nLANGFUSE_SECRET_KEY=your_langfuse_secret_key\nLANGFUSE_PUBLIC_KEY=your_langfuse_public_key\nLANGFUSE_HOST=https://cloud.langfuse.com\n\n# WhatsApp Business API\nWHATSAPP_ACCESS_TOKEN=your_whatsapp_access_token\nDEMO_PHONE_NUMBER_ID=your_phone_number_id\nDEMO_BUSINESS_ACCOUNT_ID=your_business_account_id\nMETA_VERIFY_TOKEN=verify_token\n\n# Database Configuration\nPOSTGRES_HOST\nPOSTGRES_PORT\nPOSTGRES_USER\nPOSTGRES_PASSWORD=your_postgres_password\nPOSTGRES_DB=whatsapp\n\n# Redis Configuration\nREDIS_HOST\nREDIS_PORT\nREDIS_DB\nREDIS_USERNAME=your_redis_username\nREDIS_PASSWORD=your_redis_password\n\n# Test Configuration\nTEST_PHONE_NUMBER\n</code></pre>"},{"location":"agent-build/setup/#running-the-application","title":"Running the Application","text":"<p>PatternAI Agent can be run in different modes depending on your needs:</p>"},{"location":"agent-build/setup/#method-1-using-vs-code-launch-configurations-recommended","title":"Method 1: Using VS Code Launch Configurations (Recommended)","text":"<p>The project includes pre-configured launch settings for VS Code that make development easier:</p> <ol> <li>Open the \"Run and Debug\" sidebar (Ctrl+Shift+D or Cmd+Shift+D)</li> <li> <p>Select the desired configuration from the dropdown:</p> <ul> <li>Launch Demo UI - Starts Streamlit interface</li> <li>Launch API Server - Starts backend API</li> <li>Launch UI + API - Starts both components</li> </ul> </li> <li> <p>Click the green play button or press F5</p> </li> </ol>"},{"location":"agent-build/setup/#method-2-command-line-execution","title":"Method 2: Command Line Execution","text":"<p>Run both UI+API</p> <pre><code># Terminal 1: Start API Server\npoetry run python -m clients.demo.server --local\n\n# Terminal 2: Start Demo UI\npoetry run streamlit run clients/demo/app.py\n</code></pre>"},{"location":"agent-build/setup/#development-tools","title":"Development Tools","text":"<p>The following development tools are configured in the Poetry environment to ensure high code quality, maintainability, and productivity.</p> Tool Command Purpose Type Checking <code>poetry run mypy .</code> Static type analysis Linting <code>poetry run ruff check .</code> Code quality checks Testing <code>poetry run pytest</code> Run unit &amp; integration tests"},{"location":"agent-build/setup/#managing-dependencies","title":"Managing Dependencies","text":""},{"location":"agent-build/setup/#adding-dependencies","title":"Adding Dependencies","text":"<p>Add a new package: <pre><code>poetry add package-name\n</code></pre></p> <p>To add a package with specific version: <pre><code>poetry add \"package-name&gt;=1.0.0\"\n</code></pre></p>"},{"location":"agent-build/setup/#removing-dependencies","title":"Removing Dependencies","text":"<p>To remove a package: <pre><code>poetry remove package-name\n</code></pre></p>"},{"location":"agent-build/setup/#updating-dependencies","title":"Updating Dependencies","text":"<p>To update all dependencies: <pre><code>poetry update\n</code></pre></p> <p>To update a specific package: <pre><code>poetry update package-name\n</code></pre> </p> <p>PatternAI Starter Pack</p> <p>Quick take on \u2014 User documentation Peek into the system \u2014 Developer documentation</p> <p> </p>"},{"location":"agent-build/whatsapp/","title":"PatternAI: WhatsApp Node","text":"<p>The WhatsApp Integration module extends the PatternAI Agent framework to support seamless, multi-modal conversational interactions through WhatsApp. It uses FastAPI to handle incoming webhook events from the Meta WhatsApp Business API, integrating tightly with the agent\u2019s LLM, STT, and TTS services.</p>"},{"location":"agent-build/whatsapp/#system-architecture","title":"System Architecture","text":"<p>The module is structured for cloud-native deployment and includes:</p>"},{"location":"agent-build/whatsapp/#development-environment","title":"Development Environment","text":"<ul> <li>Source Code: A FastAPI-based server to process WhatsApp messages via <code>/webhook</code>.</li> <li>Dockerfile: Defines the container definition for packaging the application.</li> <li>Configuration: Environment variables and Meta API credentials for secure operations.</li> </ul>"},{"location":"agent-build/whatsapp/#deployment-workflow","title":"Deployment Workflow","text":"<ol> <li>Docker Build \u2013 Container image is created from the source.  </li> <li>Push to Registry \u2013 Stores the image in Google Container Registry.  </li> <li>Deploy \u2013 Creates or updates the Cloud Run service.</li> </ol>"},{"location":"agent-build/whatsapp/#google-cloud-platform-infrastructure","title":"Google Cloud Platform Infrastructure","text":"<ul> <li> <p>Container Registry:   Stores Docker images at gcr. Provides versioned image storage for rollbacks. Integrates with Cloud Run for automatic deployments.</p> </li> <li> <p>Cloud Run:   Container runs the WhatsApp webhook service. Exposes <code>/webhook</code> endpoint for Meta events. Environment variables store Meta tokens and configuration.</p> </li> <li> <p>Cloud Logging:   Centralized logging for observability.</p> </li> </ul> <p></p>"},{"location":"agent-build/whatsapp/#message-processing-flow","title":"Message Processing Flow","text":"<p>Incoming WhatsApp messages go through a structured pipeline:</p>"},{"location":"agent-build/whatsapp/#components","title":"Components:","text":"<ul> <li><code>demo_whatsapp.py</code>: Entry point for handling FastAPI webhook logic.  </li> <li><code>whatsapp_base.py</code>: Base class that contains AI integration logic (LLM, STT, TTS).</li> </ul>"},{"location":"agent-build/whatsapp/#sequence-of-operations","title":"Sequence of Operations","text":"Step Operation 1 User sends a message (text or audio) to the registered WhatsApp Business number. 2 Meta sends a webhook event with the message data via an HTTP POST request to <code>/webhook</code> on our FastAPI server. 3 Webhook Handler (<code>handle_incoming_message</code>) in WhatsApp class (from <code>demo_whatsapp.py</code>, inheriting <code>WhatsAppBase</code>) receives and logs the incoming data, ignoring non-user status updates. 4 Parse and Validate message using <code>parse_webhook_event()</code> from <code>WhatsAppBase</code>. 5 Deduplication check against Redis using <code>is_duplicate_message(message_id)</code>. If duplicate: <code>{\"status\": \"duplicate\", \"message_id\": message_id}</code>. If new: mark as processed. 6 Message Type Detection to route message to either <code>process_text_message</code> (for text) or <code>process_audio_message</code> (for voice)."},{"location":"agent-build/whatsapp/#message-handling","title":"Message Handling","text":""},{"location":"agent-build/whatsapp/#text-message-flow","title":"Text Message Flow","text":"<ol> <li><code>_call_llm()</code> processes the text via LLM.  </li> <li><code>_extract_message()</code> retrieves the LLM\u2019s reply.  </li> <li><code>_call_tts()</code> synthesizes the reply to audio.  </li> <li><code>send_audio_message()</code> delivers audio back to the user.</li> </ol>"},{"location":"agent-build/whatsapp/#audio-message-flow","title":"Audio Message Flow","text":"<ol> <li><code>download_media()</code> retrieves the audio file.  </li> <li><code>_call_stt()</code> transcribes the audio.  </li> <li><code>_call_llm()</code> generates a response.  </li> <li><code>_call_tts()</code> converts it to audio.  </li> <li><code>send_audio_message()</code> returns the audio reply.</li> </ol>"},{"location":"agent-build/whatsapp/#api-endpoint","title":"API Endpoint","text":"Endpoint Method Description <code>/webhook</code> POST Receives and processes WhatsApp events <p>On failure, a fallback message is sent: <code>Sorry, we couldn't process your request. Please try again later</code>. This ensures the system degrades gracefully under failure conditions.</p>"},{"location":"agent-build/whatsapp/#logging-observability","title":"Logging &amp; Observability","text":"<ul> <li> <p>Centralized logging via Cloud Logging</p> <p>Tracks :-</p> </li> <li> <p>Incoming webhook events  </p> </li> <li>AI processing  </li> <li>Outgoing replies  </li> <li>Deduplication logic  </li> <li>Processing errors</li> </ul>"},{"location":"agent-test/","title":"AgentTest: From Prompt To Performance","text":""},{"location":"agent-test/#revolutionizing-how-teams-test-evaluate-and-perfect-their-conversational-ai","title":"REVOLUTIONIZING HOW TEAMS TEST, EVALUATE, AND PERFECT THEIR CONVERSATIONAL AI","text":"<p>Quality assurance for conversational AI agents presents critical challenges: manual test creation scales poorly, evaluation criteria lack consistency, and comprehensive edge case coverage remains difficult to achieve systematically. Development teams face hurdles in validating agent behavior across diverse scenarios while maintaining objective assessment standards.</p> <p>Imagine handing your AI agent to an intelligent testing system that operates autonomously \u2014 generating its own test cases, judging responses with expert precision, and delivering clear, actionable insights.</p> <p>That's exactly what AgentTest delivers.</p> <p>One prompt is all it takes \u2014  AgentTest handles the rest</p> <p>Creator \u201cGenerating varied     and high impact     test cases\u2013automatically\u201d</p> <p>Judge \u201cEvaluating every response     across multiple performance     metrics \u2013 with precision\u201d</p> <p>Analyst \u201cPresenting results     as a clear, interactive     report you can act on\u201d</p> <p>Onward, AgentTest expands on these roles with a fully automated testing pipeline \u2014 taking your agent\u2019s prompt and transforming it into a comprehensive suite of test cases, spanning everyday requests to edge\u2011case scenarios in multiple languages. By structuring all outputs in JSON, it ensures compatibility with automated evaluation, debugging, and continuous improvement workflows.</p> <p>Beyond generating test cases, AgentTest also serves as a judge for your AI agents. Each response from the agent is evaluated using a Langfuse\u2011driven scoring rubric, which measures performance across key dimensions such as language quality, requirement coverage, tone appropriateness, domain relevance, factual accuracy, and task fulfillment. This structured evaluation transforms subjective assessments into quantifiable, reproducible results, allowing teams to pinpoint strengths and areas for improvement. </p> <p>The framework further evolves into an analyst, distilling test outcomes into an interactive, insight\u2011driven dashboard. It displays test results via visual reports, pass/fail summaries, and detailed feedback for each case. Whether you're validating changes or tracking regressions, AgentTest provides the clarity and confidence you need to iterate fast and improve effectively.     </p> <p>\u201cTogether, these capabilities position AgentTest as a unified, end\u2011to\u2011end testing pipeline for seamless agent validation\u201d</p>"},{"location":"agent-test/#the-architecture-behind-the-roles","title":"The Architecture Behind the Roles","text":"<p>Thus far, we\u2019ve explored the conceptual architecture \u2014 now let\u2019s map it to the actual Python modules, going beyond high\u2011level concepts into implementation.</p>"},{"location":"agent-test/#prompt-management-with-langfuse","title":"Prompt Management with Langfuse","text":"<p>LangfusePromptManagement helps to version control and manage prompts collaboratively in one place. It\u2019s where teams (both technical and otherwise) create, organize, and iterate on prompts without digging into the codebase.</p> <p>The prompt management dashboard lists all prompts by name and version. From there we can create, edit, label, and version-control prompts directly through the web console.</p> <p>In our project, we integrated Langfuse to fetch prompts dynamically at runtime using a dedicated LangfusePromptManager class (defined in langfuse_prompt_manager.py). This class initializes a Langfuse client using secure environment variables and provides a simple method to retrieve prompts by name.</p> <p>By using this setup, prompt logic remains completely decoupled from application logic, making it easy to iterate on prompts during development and testing \u2014 without needing to redeploy the system. Any changes made on the Langfuse dashboard are reflected instantly in the agent behavior during test generation or evaluation.</p> <p>Here\u2019s a simplified view of how the prompt is retrieved in our code: <pre><code># Fetching a prompt from Langfuse\nprompt_manager = LangfusePromptManager()\nprompt = prompt_manager.get_prompt(\"your_prompt_name\")\n</code></pre> This integration has significantly improved flexibility and collaboration during prompt tuning, while maintaining a clean separation of responsibilities within the codebase.</p>"},{"location":"agent-test/#agent_testpy-test-case-generation-prompt-validation","title":"<code>/agent_test.py</code> \u2013 Test Case Generation &amp; Prompt Validation","text":"<p>As introduced earlier, This module handles the automated generation of test cases \u2014 implemented in agent_test.py. It begins by fetching a reusable test case template from Langfuse through LangfusePromptManager, dynamically fills it with the agent\u2019s configuration prompt and desired number of cases, and then invokes an LLM to generate structured input\u2013output examples.</p>"},{"location":"agent-test/#code-snippets","title":"Code snippets:","text":"<p>Template retrieval</p> <p>The generator retrieves a Langfuse-stored prompt template using LangfusePromptManager: <pre><code>template = prompt_manager.get_prompt(\"agent_test\")\n</code></pre> Template filling &amp; llm invocation</p> <p>The agent prompt and test case count are dynamically inserted into the template: <pre><code>filled_prompt = fill_prompt(template, {\n    \"agent_prompt\": agent_prompt,\n    \"num_cases\": n,\n})\nllm_output = call_llm(filled_prompt)\n</code></pre> Robust json extraction</p> <p>LLM outputs are often noisy or malformed. To handle this, the module implements a recovery parser: <pre><code>def extract_json(text: str) -&gt; list:\n    # Attempts clean JSON parsing\n    # Falls back to partial extraction of valid objects\n</code></pre></p> <p>\u201cTo maintain test diversity and prevent redundancy, the module derives a unique signature for each generated case and filters    out duplicates before saving. This ensures that every test case contributes new coverage rather than repeating existing ones, making the generated suite more comprehensive and realistic for agent evaluation.\u201d</p>"},{"location":"agent-test/#agent_evalpy-llmdriven-evaluation-scoring","title":"<code>/agent_eval.py</code> \u2013 LLM\u2011Driven Evaluation &amp; Scoring","text":"<p>This module acts as an autonomous evaluation engine, using a Langfuse\u2011managed evaluation template and an LLM\u2011based judge to score agent responses against user expectations. It evaluates multiple dimensions \u2014 language quality, requirement coverage, tone, domain consistency, factual accuracy, and task fulfillment \u2014 producing a structured, explainable JSON report for each test case.</p>"},{"location":"agent-test/#code-snippets_1","title":"Code snippets:","text":"<p>Evaluation template retrieval</p> <p>An evaluation prompt is fetched from Langfuse to ensure consistent, centrally managed criteria: <pre><code>template = prompt_manager.get_prompt(\"agent_eval\")\n</code></pre></p> <p>Dynamic context injection</p> <p>The test case\u2019s user input and the agent\u2019s response are injected into the template before sending to the LLM: <pre><code>filled_prompt = fill_prompt_template(template, {\n    \"user_input\": user_input,\n    \"llm_response\": llm_response\n})\n</code></pre></p> <p>LLM\u2011based judgement &amp; json parsing</p> <p>The LLM produces a structured evaluation, which is then cleaned and parsed: <pre><code>raw_output = call_llm(filled_prompt)\ncleaned_output = strip_markdown_json(raw_output)\nevaluation = json.loads(cleaned_output)\n</code></pre></p>"},{"location":"agent-test/#inside-the-logic","title":"Inside the logic:","text":"<ul> <li>Date\u2011aware reasoning: Interprets relative dates (e.g., \u201ctomorrow\u201d, \u201cnext weekend\u201d) based on the current date ({{today}}) and penalizes incorrect interpretations.</li> <li>Hallucination detection: It flags responses where the agent fabricates details instead of requesting clarification.</li> <li>Error\u2011tolerant parsing: If the LLM returns malformed JSON, the module logs the raw response and gracefully recovers, ensuring no evaluation is silently lost.</li> <li>Explainable scoring: Each dimension includes a score and justification, making the evaluation transparent and actionable for developers.</li> </ul>"},{"location":"agent-test/#heres-how-you-can-start-testing-with-agenttest-ready","title":"Here\u2019s how you can start testing with AgentTest \u2014 READY?","text":"<p>Follow these steps :</p> <ol> <li> <p>Upload your agent configuration prompt.</p> </li> <li> <p>Confirm its validity </p> </li> </ol> <pre><code>Given a prompt is submitted for evaluation \nWhen the system checks the prompt \nThen the prompt must satisfy the following: \n- The prompt must include enough context or rules for response generation \n- The length of the prompt must be at least 100 characters \n- The prompt must be written in a supported and properly formatted language \n</code></pre> <p>Note</p> <p>If the prompt does not meet these conditions, the prompt will be rejected.</p>"},{"location":"agent-test/#what-youll-see","title":"What You\u2019ll See","text":"<p>Once the evaluation begins, AgentTest provides,</p> <ul> <li>Visual metrics dashboard showing pass/fail breakdown.</li> <li>Individual test case insights \u2014 showing what failed and why.</li> <li>Downloadable JSON reports: For deeper inspection or integration into QA pipelines.</li> </ul>"},{"location":"agent-test/#passing-parameters","title":"Passing Parameters","text":"<p>A test is considered passed when,</p> <ul> <li>All metric scores are consistently high (typically \u22654/5).</li> <li>The Judge\u2019s final verdict is \"overall_judgment\": \"Pass\"</li> </ul> <p><pre><code>Given an agent response is evaluated\nWhen the Judge LLM analyzes the output\nThen it is scored based on:\n  - Language Quality: checks grammar and fluency\n  - Requirement Coverage: ensures all input details are addressed\n  - Tone Appropriateness: assesses politeness and helpfulness\n  - Domain Appropriateness: confirms relevance to agent\u2019s role\n  - Factual Accuracy: flags hallucinated or incorrect content\n  - Task Fulfillment: evaluates whether the user\u2019s goal is achieved\n</code></pre> Want to see it in action?  Watch the video </p>"},{"location":"agent-test/setup/","title":"Installation Guide","text":""},{"location":"agent-test/setup/#installation-guide","title":"Installation Guide","text":"<p>Follow these steps to set up and run the project using Poetry.</p>"},{"location":"agent-test/setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/PatternAILabs/patternai-agent-build.git\ncd patternai-agent-build\n</code></pre>"},{"location":"agent-test/setup/#2-install-poetry-if-not-already-installed","title":"2. Install Poetry (if not already installed)","text":"<p>For macOS/Linux: <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre></p> <p>For Windows (using PowerShell): <pre><code>(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\n</code></pre></p>"},{"location":"agent-test/setup/#3-configure-poetry-for-in-project-virtual-environments","title":"3. Configure Poetry for In-Project Virtual Environments","text":"<pre><code>poetry config virtualenvs.in-project true\n</code></pre>"},{"location":"agent-test/setup/#4-initialize-and-install-dependencies","title":"4. Initialize and Install Dependencies","text":"<pre><code>poetry install\n</code></pre>"},{"location":"agent-test/setup/#5-activate-the-virtual-environment","title":"5. Activate the Virtual Environment","text":"<p>For macOS/Linux: <pre><code>source $(poetry env info --path)/bin/activate\n</code></pre></p> <p>For Windows: <pre><code>.venv\\Scripts\\activate\n</code></pre></p>"},{"location":"agent-test/setup/#6-set-up-environment-variables","title":"6. Set Up Environment Variables","text":"<p>Create a <code>.env</code> file in the project root with the following variables:</p> <pre><code># AI Service APIs\nOPENROUTER_API_KEY=your_openrouter_key\nLLM_MODEL= model_used\n# Langfuse Configuration\nLANGFUSE_SECRET_KEY=your_langfuse_secret_key\nLANGFUSE_PUBLIC_KEY=your_langfuse_public_key\nLANGFUSE_HOST=https://cloud.langfuse.com\n</code></pre>"},{"location":"agent-test/setup/#running-the-application","title":"Running the Application","text":"<p>PatternAI Agenttest can be run using the command : <pre><code>poetry run streamlit run app.py\n</code></pre></p>"},{"location":"agent-test/setup/#development-tools","title":"Development Tools","text":"<p>The following development tools are configured in the Poetry environment to ensure high code quality, maintainability, and productivity.</p> Tool Command Purpose Type Checking <code>poetry run mypy .</code> Static type analysis Linting <code>poetry run ruff check .</code> Code quality checks Testing <code>poetry run pytest</code> Run unit &amp; integration tests"},{"location":"agent-test/setup/#managing-dependencies","title":"Managing Dependencies","text":""},{"location":"agent-test/setup/#adding-dependencies","title":"Adding Dependencies","text":"<p>Add a new package: <pre><code>poetry add package-name\n</code></pre></p> <p>To add a package with specific version: <pre><code>poetry add \"package-name&gt;=1.0.0\"\n</code></pre></p>"},{"location":"agent-test/setup/#removing-dependencies","title":"Removing Dependencies","text":"<p>To remove a package: <pre><code>poetry remove package-name\n</code></pre></p>"},{"location":"agent-test/setup/#updating-dependencies","title":"Updating Dependencies","text":"<p>To update all dependencies: <pre><code>poetry update\n</code></pre></p> <p>To update a specific package: <pre><code>poetry update package-name\n</code></pre> </p>"}]}