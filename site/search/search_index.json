{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"PatternAI Agent - Build <p>streaming solutions for rising needs.</p> Get Started"},{"location":"developer/","title":"Developer Insight into PatternAI Agent","text":""},{"location":"developer/#overview","title":"Overview","text":"<p>PatternAI Agent is a modular, voice-enabled conversational agent framework built to deliver seamless, human-like interactions via audio and text channels through WhatsApp.</p> <p></p> <p>The system integrates multiple Speech-to-Text (STT) and Text-to-Speech (TTS) services, connects to powerful Large Language Models (LLMs) through OpenRouter, and leverages Langfuse for prompt tracing and monitoring. It's designed with a clean backend and modular agent profiles for domain-specific use cases like shopping, travel assistance, etc.</p>"},{"location":"developer/#project-walkthrough-diving-into-the-system","title":"Project Walkthrough: Diving into the System","text":"<p>To understand the operation of the PatternAI Agent, this section will guide through the process flow, from the initial receipt of a user's request to the generation and delivery of a response. Key system components and their interactions will be unpacked.</p>"},{"location":"developer/#the-foundation","title":"The Foundation","text":"<ul> <li><code>/app</code>: This directory houses the core application logic and services that drive the agent's functionality. It includes:</li> </ul> <p><code>/services/</code>: encapsulates the core AI processing logic of the PatternAI Agent. It defines abstract interfaces and provides concrete implementations for key AI processing capabilities.</p> <p>These capabilities are organized into the following sub-modules:</p> services/ <pre><code>services/\n\u251c\u2500\u2500 llm/\n\u2502   \u251c\u2500\u2500 __init__.py              # Factory: creates LLM handler (OpenRouter/custom)\n\u2502   \u251c\u2500\u2500 base_handler.py          # Abstract base class for LLMs (defines prompt + interface)\n\u2502   \u2514\u2500\u2500 openrouter_handler.py    # Implementation using OpenRouter API\n\u2502\n\u251c\u2500\u2500 stt/\n\u2502   \u251c\u2500\u2500 __init__.py              # Factory: selects STT engine\n\u2502   \u251c\u2500\u2500 base_stt_handler.py      # Abstract base class for STT services\n\u2502   \u251c\u2500\u2500 whisper_stt.py           # STT via OpenAI Whisper API\n\u2502   \u251c\u2500\u2500 google_stt.py            # STT via Google Cloud Speech API\n\u2502   \u2514\u2500\u2500 elevenlabs_stt.py        # STT via ElevenLabs API\n\u2502\n\u2514\u2500\u2500 tts/\n    \u251c\u2500\u2500 __init__.py              # Factory: selects TTS engine\n    \u251c\u2500\u2500 base_tts_handler.py      # Abstract TTS base class, defines text-to-audio interface\n    \u2514\u2500\u2500 edge_tts.py              # TTS implementation using Microsoft Edge TTS\n</code></pre> <p><code>/llm</code>: provides a flexible interface for interacting with various Large Language Models (LLMs) through OpenRouter, with support for prompt management and model fallback mechanisms. The module is designed to be extensible, allowing for easy integration of additional LLM providers in the future.</p> <p><code>/stt</code>: a unified interface for converting speech to text using multiple cloud providers. It supports Whisper (OpenAI), Google Cloud Speech-to-Text, and ElevenLabs services through a common abstraction layer.</p> <p><code>/tts</code>: a standardized interface for converting text to speech using various synthesis engines. Currently implemented with Microsoft Edge's TTS service, the module is designed for easy extensibility to support additional providers.</p>"},{"location":"developer/#service-traversal","title":"Service Traversal","text":"<p>The system operates through a series of coordinated steps that leverage the abstract interfaces and factory pattern to process user requests. </p> <p>When a client interacts with the system, the request follows a consistent architectural pattern across all service types (STT, LLM, and TTS):</p> <p>While processing an audio file, the system uses factory functions like <code>create_stt_service</code> to instantiate the appropriate service (Whisper, Google STT, or ElevenLabs). These factories handle configuration parsing, environment variable injection, and service initialization in one centralized location.</p> <p>After initialization, the system interacts with each service through its abstract interface. For example, calling <code>transcribe()</code> on any STT service produces a text transcript regardless of which underlying provider (Whisper, Google, or ElevenLabs) is actually processing the audio.</p> <p>The LLM processing and text-to-speech (TTS) flows follow a similar architectural pattern.</p> <ul> <li>For LLMs, the system uses <code>create_llm_handler</code> to instantiate an <code>OpenRouterHandler</code>, which manages prompt handling, fallback mechanisms, and LLM interactions.</li> <li>For TTS, the <code>create_tts_service</code> factory function initializes an EdgeTTS service that converts text input into audio bytes using the Microsoft Edge TTS engine.</li> </ul> <p>While models provide services by default, their performance can vary, a topic we'll explore using Langfuse later. Keeping that in mind, we've built a framework that allows developers to integrate models as they please.</p>"},{"location":"developer/#how-to-plug-in","title":"How to plug in:","text":"<p>For instance, let's integrate a new STT service:</p> <ul> <li>Implement a handler class inheriting from <code>BaseSTTClass</code>.</li> <li>Add it to the <code>service_registry</code> in <code>create_stt_service</code>.</li> </ul> <pre><code>service_registry = {\n    \"whisper\": WhisperSTT,\n    \"google\": GoogleSTT,\n    \"elevenlabs\": ElevenLabsSTT,\n    \"new_service\": NewSTTService  # add here\n}\n</code></pre> <p>Also we've optimized performance by implementing factory functions with LRU caching and custom cache logging decorators, preventing redundant service creation when the same configuration is requested multiple times. This allows dynamic, efficient, and configurable creation of service instances at runtime.</p> <p><code>/tests/</code>: This directory contains unit and integration tests for the application logic, ensuring the reliability and correctness of the core functionalities. Tests are executed using the pytest framework.</p> <p>To run all tests, use the following command:</p> <pre><code>poetry run pytest\n</code></pre>"},{"location":"developer/#test-structure","title":"Test Structure","text":"<p>The <code>tests/</code> directory is organized by module, with each file typically containing tests for a specific part of the application:</p> <ul> <li><code>test_llm.py</code>: Contains tests for the Large Language Model (LLM) integration, including schema validation and semantic comparison of LLM outputs</li> <li><code>test_stt.py</code>: Contains tests for the Speech-to-Text (STT) service, covering different STT providers and transcription functionality</li> <li><code>test_tts.py</code>: Contains tests for the Text-to-Speech (TTS) service, including service creation, configuration, and speech synthesis</li> <li><code>conftest.py</code>: Provides helper functions and fixtures used across multiple test files. For example, the <code>cleaned_json</code> function normalizes JSON outputs from LLMs, which is crucial for testing LLM responses</li> </ul>"},{"location":"developer/#versatility-and-modularity","title":"Versatility and Modularity","text":"<ul> <li><code>/clients</code>: provides a flexible framework for implementing domain-specific conversational agents with WhatsApp integration. Each client gets its own isolated configuration and implementation while sharing core AI services (STT/TTS/LLM).</li> </ul>"},{"location":"developer/#api-endpoints","title":"API Endpoints","text":"<p>With the core services for STT, TTS, and LLM dynamically monitored through our modular framework, these capabilities are made accessible to client applications via a set of clean, RESTful API endpoints. These endpoints handle everything from speech transcription to language and audio generation, providing a unified interface for seamless multi-modal conversations.</p> <p>The service exposes the following endpoints:</p> Endpoint Method Description <code>/health</code> GET Health check endpoint <code>/stt</code> POST Speech-to-Text transcription <code>/llm</code> POST Process text with LLM models <code>/tts</code> POST Text-to-Speech synthesis"},{"location":"developer/#health-check","title":"Health Check","text":"<p>Verify if the service is running:</p> <pre><code>curl -X POST \"http://localhost:5000/health\"\n</code></pre> <p>Response: <pre><code>{\"status\": \"ok\"}\n</code></pre></p>"},{"location":"developer/#1-speech-to-text-stt","title":"1. Speech-to-Text (STT)","text":"<p>Convert audio to text using the <code>/stt</code> endpoint.</p> <p>Request</p> <pre><code>curl -X POST \"http://localhost:5000/stt\" \\\n  -H \"Content-Type: application/octet-stream\" \\\n  -H \"STT-Model: whisper\" \\\n  --data-binary @audio_file.webm\n</code></pre> <p>Available STT models: - <code>whisper</code> (OpenAI's Whisper model) - <code>google</code> (Google Speech-to-Text) - <code>elevenlabs</code> (ElevenLabs STT service)</p> <p>The model can be selected via the <code>STT-Model</code> header. If not specified, the default model configured in <code>configs.yaml</code> will be used.</p> <p>Response: <pre><code>{\n  \"status\": \"success\",\n  \"transcription\": \"This is the transcribed text from the audio.\",\n  \"request_id\": \"unique-request-id\"\n}\n</code></pre></p>"},{"location":"developer/#2-language-model-llm-processing-sub-heading","title":"2. Language Model (LLM) Processing{: .sub-heading}","text":"<p>Process transcribed text with an LLM model using the <code>/llm</code> endpoint.</p> <p>Request</p> <pre><code>curl -X POST \"http://localhost:5000/llm\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"transcription\": \"What items do you have on sale?\",\n    \"model_name\": \"anthropic/claude-3-haiku-20240307\",\n    \"agent_id\": \"agent_shopping\"\n  }'\n</code></pre> <p>Notes: - <code>model_name</code>: Specifies which LLM model to use (required if <code>fallback_enabled</code> is set to false in configuration) - <code>agent_id</code>: Optional parameter to specify which agent profile to use (e.g., \"agent_shopping\", \"agent_travel\")</p> <p>Response:</p> <p>The response is streamed in plain text format:</p> <pre><code>request_id: unique-request-id\n\nWe have several items on sale this week. Our fresh produce section has discounts on seasonal fruits and vegetables. In the grocery aisle, we have deals on pasta, rice, and canned goods. The bakery section has a buy-one-get-one offer on whole wheat bread. Is there anything specific you're looking for?\n</code></pre>"},{"location":"developer/#3-text-to-speech-tts","title":"3. Text-to-Speech (TTS)","text":"<p>Convert text to speech using the <code>/tts</code> endpoint.</p> <p>Request</p> <pre><code>curl -X POST \"http://localhost:5000/tts\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"TTS-Model: edge_tts\" \\\n  -d '{\n    \"message\": \"Hello, we have several items on sale this week. Is there anything specific you'\\''re looking for?\"\n  }' \\\n  --output speech.mp3\n</code></pre> <p>Available TTS models: - <code>edge_tts</code> (Microsoft Edge TTS)</p> <p>Response:</p> <p>The response is an audio file in MP3 format with appropriate headers:</p> <pre><code>Content-Type: audio/mpeg\nContent-Disposition: attachment; filename=speech.mp3\nX-Request-ID: unique-request-id\n</code></pre>"},{"location":"developer/#langfuse-for-prompt-management","title":"Langfuse For Prompt Management","text":"<p>\"For a transparent view into the agent's language-driven interactions and a pathway to iterative improvement, the framework leverages Langfuse for robust prompt management and comprehensive monitoring.\"</p> <p>To ensure that PatternAI Agent remains highly modular and adaptable, prompt management is architected with a clear separation of concerns.</p> <p>We've established an abstract Base Interface (<code>BasePromptManager</code>) - defined in <code>/base_prompt_handler.py</code>, which serves as an abstract interface for prompt retrieval. This allows us to seamlessly integrate various prompt management solutions without impacting the core logic that interacts with LLMs.</p> <p>Our current implementation LangfusePromptManager - defined in <code>/langfuse_handler.py</code>, leverages the Langfuse SDK to dynamically fetch prompt definitions from a centralized prompt store. This approach offers several key advantages:</p> <ul> <li>Centralized Prompt Management: All prompt templates are managed within Langfuse, providing a single source of truth and simplifying updates.</li> <li>Dynamic Updates: Changes to prompts in Langfuse are reflected in the agent without requiring code deployments.</li> <li>Version Control &amp; History: Langfuse provides built-in versioning and history tracking for prompts, crucial for experimentation and rollback.</li> <li>Observability: Langfuse integrates seamlessly with prompt tracing and monitoring, offering valuable insights into prompt performance and usage.</li> </ul> <p></p> <p></p> <p></p> <p>The <code>PromptManager</code> interface dictates that any concrete implementation must provide a <code>get_prompt(name)</code> method. Our <code>LangfusePromptManager</code> fulfills this contract by initializing the Langfuse SDK using environment variables (<code>LANGFUSE_PUBLIC_KEY</code>, <code>LANGFUSE_SECRET_KEY</code>, <code>LANGFUSE_HOST</code>) and retrieving prompts by their unique name using the Langfuse client:</p> <pre><code>self.client.get_prompt(name)\n</code></pre> <p>This decoupling empowers us to evolve our prompt management strategy in the future \u2013 simply by implementing a new class that adheres to the <code>PromptManager</code> interface. This design ensures that the PatternAI Agent remains flexible and maintainable as our requirements evolve.</p> <p>Note</p> <p>Want to customize or extend the monitoring?  You can easily plug in your own prompt management or analytics solution too.  To learn more about Langfuse, visit their official documentation (https://langfuse.com/docs).</p>"},{"location":"developer/#langfuse-for-tracking","title":"Langfuse For Tracking","text":"<p>To gain precise observability into the performance and behavior of our multi-modal services \u2013 including language modeling (LLM), speech-to-text (STT), and text-to-speech (TTS) \u2013 we integrate Langfuse. This provides structured, end-to-end tracing of interactions across services, enabling detailed logging of input/output data, metadata, cost, and latency for each API call in our application pipeline.</p>"},{"location":"developer/#why-we-use-it-here","title":"Why We Use It Here","text":"<p>In multi-modal AI architectures, especially those combining LLM, STT, and TTS services, monitoring individual service calls and aggregating their performance and cost metrics can be complex. Langfuse simplifies this by:</p> <ul> <li>Enabling real-time cost tracking per API call.</li> <li>Capturing performance metrics such as latency, token usage, audio duration, and processing time.</li> <li>Providing unified tracing across services, allowing correlation of input/output pairs and their associated metadata.</li> <li>Supporting prompt versioning and retrieval analytics for LLM generations.</li> </ul>"},{"location":"developer/#langfuse-integration-usage","title":"Langfuse Integration &amp; Usage","text":"<p>To enable request tracing and detailed observability across all AI components, Langfuse is initialized using credentials securely stored in environment variables.</p> <p>Langfuse's SDK is initialized in each tracker with:</p> <pre><code>self.langfuse = Langfuse(\n    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n    host=os.getenv(\"LANGFUSE_HOST\")\n)\n</code></pre> <p>This Langfuse instance is then used throughout the application to track operations and metrics for each service.</p>"},{"location":"developer/#example-usage","title":"Example Usage","text":"<p>Langfuse tracing relies primarily on two methods:</p> <ul> <li><code>trace()</code> is used to start a new tracing session, encapsulating the full flow of a request.</li> <li><code>generation()</code> records specific input/output events, such as transcriptions, responses, or synthesized speech, including performance and cost metadata.</li> </ul> <p>Example: Starting a trace</p> <pre><code>trace = self.langfuse.trace(\n    name=\"cost_analysis\",\n    user_id=user_id,\n    metadata={\"environment\": os.getenv(\"ENV\", \"dev\"), \"source\": \"speech_input\"}\n)\n</code></pre> <p>Example: Logging a generation event</p> <pre><code>trace.generation(\n    name=f\"stt-transcription-{model_name}\",\n    input=f\"[Audio duration: {audio_duration_sec:.2f}s]\",\n    output=transcription,\n    model=model_name,\n    metadata={\n        \"duration_audio\": round(audio_duration_sec, 8),\n        \"cost\": round(cost, 8),\n        \"currency\": \"USD\",\n    }\n)\n</code></pre>"},{"location":"developer/#service-specific-trackers","title":"Service specific trackers","text":"<p>We encapsulate Langfuse functionality within dedicated tracker classes for each core service.</p> <p>LLMTracker \u2192 <code>/clients/demo/llm_tracker.py</code></p> <p>Handles all tracing related to LLM interactions. It initializes with model-specific configuration such as token pricing and supported providers from <code>configs.yaml</code>, starts a trace with <code>start_trace()</code>, and records generation event with Langfuse using <code>track_llm()</code>.</p>"},{"location":"developer/#metrics-tracked","title":"Metrics Tracked:","text":"<ul> <li>Input/output tokens</li> <li>Latency and response time</li> <li>Cost per model/token type</li> <li>Prompt metadata</li> </ul> <p>STTTracker \u2192 <code>/clients/demo/stt_tracker.py</code></p> <p>Manages speech-to-text tracing. It loads model and pricing info, starts a trace via <code>start_trace()</code>, and logs the transcription with <code>track_stt()</code>.</p>"},{"location":"developer/#metrics-tracked_1","title":"Metrics Tracked:","text":"<ul> <li>Audio duration</li> <li>Transcription text</li> <li>STT model used</li> <li>Cost per second</li> </ul> <p>TTSTracker \u2192 <code>/clients/demo/tts_tracker.py</code></p> <p>Tracks text-to-speech operations. At initialization, it reads the default voice model and pricing scheme\u2014covering both per-character and per-second billing\u2014from the config, starts a trace with <code>start_trace()</code>, and logs synthesis details in <code>track_tts()</code>.</p>"},{"location":"developer/#metrics-tracked_2","title":"Metrics Tracked:","text":"<ul> <li>Character count</li> <li>Audio duration</li> <li>Processing time</li> <li>Model and voice used</li> <li>Combined cost (char + audio-based)</li> </ul> <p>All configuration values \u2014 such as pricing, model names, and default services \u2014 are centrally managed via <code>configs.yaml</code>.</p> <p>After each conversation, the metrics are calculated and visualized in Langfuse's dashboard, where teams monitor cost-per-session or drill into LLM token usage by model. </p> <p></p>"},{"location":"developer/#whatsapp-integration","title":"WhatsApp Integration","text":"<p>Support for WhatsApp messaging is currently being developed to enable seamless voice and text interactions through the platform.  Hop into Whatsapp Integration.</p> <p>This document has provided a comprehensive overview of the PatternAI Agent, a modular framework for building voice-enabled conversational agents. We covered the system's architecture, key components, and how they interact to process user requests. The agent's flexibility is enabled by its modular design, support for multiple STT, TTS, and LLM providers, and the use of Langfuse for detailed observability.</p>"},{"location":"developer/#next-steps","title":"Next Steps","text":"<p>To get started with the PatternAI Agent:</p> <ol> <li>Ensure you have met all the system requirements.</li> <li>Follow the instructions in the Installation Guide to set up the environment and dependencies.</li> <li>Explore the example clients in the <code>clients/</code> directory.</li> </ol>"},{"location":"developer/#resources","title":"Resources","text":"<ul> <li>GitHub Repository: PatternAI Agent</li> </ul> <p>We encourage you to leverage the PatternAI Agent to create innovative and engaging conversational experiences. We welcome your feedback and contributions!</p> <p></p>"},{"location":"prerequisites/","title":"Prerequisites","text":""},{"location":"prerequisites/#prerequisites","title":"Prerequisites","text":"<p>Before installing and running the application, ensure the following:</p> <ul> <li>Python 3.12+ - (Required due to version-specific features)</li> <li>Poetry - (For dependency and environment management)</li> <li>API Keys for external services:<ul> <li>OpenAI (for Whisper STT)</li> <li>Google Cloud (for Google STT)</li> <li>ElevenLabs (for ElevenLabs STT)</li> <li>OpenRouter (for LLM access)</li> <li>Langfuse (for prompt management and tracing)</li> </ul> </li> </ul>"},{"location":"setup/","title":"Installation Guide","text":"<p>Pattern AI agent is a versatile, voice enabled agent designed to enhance and streamline how users converse with digital services across multiple domains. These interactions remain human-like, with tightly tailored responses that allow the agent to serve as a true partner in meeting users needs and requests.</p>"},{"location":"setup/#installation-guide","title":"Installation Guide","text":"<p>Follow these steps to set up and run the project using Poetry.</p>"},{"location":"setup/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/PatternAILabs/patternai-agent-build.git\ncd patternai-agent-build\n</code></pre>"},{"location":"setup/#2-install-poetry-if-not-already-installed","title":"2. Install Poetry (if not already installed)","text":"<p>For macOS/Linux: <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre></p> <p>For Windows (using PowerShell): <pre><code>(Invoke-WebRequest -Uri https://install.python-poetry.org -UseBasicParsing).Content | python -\n</code></pre></p>"},{"location":"setup/#3-configure-poetry-for-in-project-virtual-environments","title":"3. Configure Poetry for In-Project Virtual Environments","text":"<pre><code>poetry config virtualenvs.in-project true\n</code></pre>"},{"location":"setup/#4-initialize-and-install-dependencies","title":"4. Initialize and Install Dependencies","text":"<pre><code>poetry install\n</code></pre>"},{"location":"setup/#5-activate-the-virtual-environment","title":"5. Activate the Virtual Environment","text":"<p>For macOS/Linux: <pre><code>source $(poetry env info --path)/bin/activate\n</code></pre></p> <p>For Windows: <pre><code>.venv\\Scripts\\activate\n</code></pre></p>"},{"location":"setup/#6-set-up-environment-variables","title":"6. Set Up Environment Variables","text":"<p>Create a <code>.env</code> file in the project root with the following variables:</p> <pre><code># AI Service APIs\nOPENROUTER_API_KEY=your_openrouter_key\nOPENAI_API_KEY=your_openai_key_here\nELEVENLABS_API_KEY=your_elevenlabs_key\n\n# Google Cloud Credentials (JSON format)\nGOOGLE_APPLICATION_CREDENTIALS={\"type\": \"service_account\", \"project_id\": \"your-project-id\", ...}\n\n# Langfuse Configuration\nLANGFUSE_SECRET_KEY=your_langfuse_secret_key\nLANGFUSE_PUBLIC_KEY=your_langfuse_public_key\nLANGFUSE_HOST=https://cloud.langfuse.com\n\n# WhatsApp Business API\nWHATSAPP_ACCESS_TOKEN=your_whatsapp_access_token\nDEMO_PHONE_NUMBER_ID=your_phone_number_id\nDEMO_BUSINESS_ACCOUNT_ID=your_business_account_id\nMETA_VERIFY_TOKEN=verify_token\n\n# Database Configuration\nPOSTGRES_HOST\nPOSTGRES_PORT\nPOSTGRES_USER\nPOSTGRES_PASSWORD=your_postgres_password\nPOSTGRES_DB=whatsapp\n\n# Redis Configuration\nREDIS_HOST\nREDIS_PORT\nREDIS_DB\nREDIS_USERNAME=your_redis_username\nREDIS_PASSWORD=your_redis_password\n\n# Test Configuration\nTEST_PHONE_NUMBER\n</code></pre>"},{"location":"setup/#running-the-application","title":"Running the Application","text":"<p>PatternAI Agent can be run in different modes depending on your needs:</p>"},{"location":"setup/#method-1-using-vs-code-launch-configurations-recommended","title":"Method 1: Using VS Code Launch Configurations (Recommended)","text":"<p>The project includes pre-configured launch settings for VS Code that make development easier:</p> <ol> <li>Open the \"Run and Debug\" sidebar (Ctrl+Shift+D or Cmd+Shift+D)</li> <li> <p>Select the desired configuration from the dropdown:</p> <ul> <li>Launch Demo UI - Starts Streamlit interface</li> <li>Launch API Server - Starts backend API</li> <li>Launch UI + API - Starts both components</li> </ul> </li> <li> <p>Click the green play button or press F5</p> </li> </ol>"},{"location":"setup/#method-2-command-line-execution","title":"Method 2: Command Line Execution","text":"<p>Run both UI+API</p> <pre><code># Terminal 1: Start API Server\npoetry run python -m clients.demo.server --local\n\n# Terminal 2: Start Demo UI\npoetry run streamlit run clients/demo/app.py\n</code></pre>"},{"location":"setup/#development-tools","title":"Development Tools","text":"<p>The following development tools are configured in the Poetry environment to ensure high code quality, maintainability, and productivity.</p> Tool Command Purpose Type Checking <code>poetry run mypy .</code> Static type analysis Linting <code>poetry run ruff check .</code> Code quality checks Testing <code>poetry run pytest</code> Run unit &amp; integration tests"},{"location":"setup/#managing-dependencies","title":"Managing Dependencies","text":""},{"location":"setup/#adding-dependencies","title":"Adding Dependencies","text":"<p>Add a new package: <pre><code>poetry add package-name\n</code></pre></p> <p>To add a package with specific version: <pre><code>poetry add \"package-name&gt;=1.0.0\"\n</code></pre></p>"},{"location":"setup/#removing-dependencies","title":"Removing Dependencies","text":"<p>To remove a package: <pre><code>poetry remove package-name\n</code></pre></p>"},{"location":"setup/#updating-dependencies","title":"Updating Dependencies","text":"<p>To update all dependencies: <pre><code>poetry update\n</code></pre></p> <p>To update a specific package: <pre><code>poetry update package-name\n</code></pre> </p> <p>PatternAI Starter Pack</p> <p>Quick take on \u2014 User documentation Peek into the system \u2014 Developer documentation</p> <p> </p>"},{"location":"user/","title":"PatternAI Agent: Voicing a Change","text":"<p>In these times, where attention is fleeting and retention is hard-won, people value speed, ease, and familiarity more than ever. User convenience is king and businesses must connect with people on the platforms they already use. PatternAI Agent meets users where they are, transforming everyday chat apps into intelligent, voice-powered assistants that feel natural and effortless. Whether it's placing an order, getting support, or booking a trip, customers can simply send a voice message or a text, and get instant, helpful responses.</p> <p>PatternAI Agent lets people talk naturally with a smart assistant on WhatsApp \u2014 using voice or text \u2014 just like chatting with a friend.</p> <p>Built with the flexibility of developers and the needs of enterprises in mind, PatternAI integrates cutting-edge AI models for speech, language, and dialogue \u2014 all within a WhatsApp-native experience.</p>"},{"location":"user/#what-makes-it-great","title":"What Makes It Great?","text":"<p>Can't type? Talk: Customers can speak naturally to a WhatsApp assistant, hands-free or type personifying it into a companion.</p> <p>Familiar &amp; Convenient: No new apps to download. Everything happens right inside WhatsApp, a platform people already love.</p> <p>Instant Replies: Get instant, streaming responses for fluid conversations.</p> <p>Always Reliable: Even in noisy or difficult situations, it keeps conversations smooth and helpful.</p>"},{"location":"user/#whos-it-for","title":"Who's It For?","text":""},{"location":"user/#1-businesses-clients-developers","title":"1. Businesses / Clients / Developers:","text":"<p>These are the people or companies who adopt and integrate PatternAI Agent into their services.</p> <p>Examples: - E-commerce platforms that want voice-based customer support. - Travel agencies offering real-time, conversational bookings via WhatsApp. - Customer support teams looking to reduce manual workloads. - Developers / system integrators building custom agents for clients.</p>"},{"location":"user/#2-end-users-consumers-customers","title":"2. End Users (Consumers / Customers):","text":"<p>These are the users of WhatsApp who interact with the PatternAI-powered agents without knowing what's behind the scenes.</p> <p>Examples: - A customer tracking their online order via voice message. - Someone booking a trip through WhatsApp by speaking to an assistant. - Someone seeking product recommendations in a native language.</p>"},{"location":"user/#lets-converse-step-by-step","title":"Let's Converse, Step by Step","text":"<p>With PatternAI, conversations on WhatsApp just flow \u2014 smart, fast, and surprisingly human.</p> <p></p> <p>When you send a voice message:</p> <ol> <li> <p>Your message is heard.    The assistant listens and understands what you said.</p> </li> <li> <p>It figures out what you need.    Whether it's booking a flight, checking an order, or answering a question \u2013 it gets the context.</p> </li> <li> <p>You get a quick, helpful reply.    The assistant responds instantly in voice \u2013 based on what works best for you.</p> </li> </ol> <p>That's it. No waiting. No typing (unless you want to). Just natural conversations.</p>"},{"location":"user/#frequently-asked-questions","title":"\u2753 Frequently Asked Questions","text":""},{"location":"user/#1-can-i-change-the-voice-of-the-assistant","title":"1. Can I change the voice of the assistant?","text":"<p>Not at the moment. The assistant uses a fixed voice that's chosen by the developers for clarity and consistency. While the system supports different voice types internally, users cannot change the voice directly. We may offer more voice options in future updates.</p>"},{"location":"user/#2-how-do-i-clear-all-my-data-from-the-assistant","title":"2. How do I clear all my data from the assistant?","text":""},{"location":"user/#3-can-i-book-an-uber-using-the-agent","title":"3. Can I book an Uber using the agent?","text":"<p>Not yet \u2014 but integrations like Uber are in our roadmap. Stay tuned for updates!</p>"},{"location":"user/#4-is-my-personal-data-shared-between-domains-or-stored-without-my-consent","title":"4. Is my personal data shared between domains or stored without my consent?","text":"<p>No. PatternAI is privacy-first. Your data is only used for that specific interaction unless you explicitly allow otherwise.</p>"},{"location":"user/#5-how-much-data-is-saved-to-the-system","title":"5. How much data is saved to the system?","text":"<p>Only the data needed to process your current request is temporarily stored. For transparency, the assistant uses Langfuse to track performance and cost, not to store personal content permanently.</p>"},{"location":"user/#6-can-i-send-a-photo-or-video-of-the-items-i-want-to-order","title":"6. Can I send a photo or video of the items I want to order?","text":"<p>For now, only voice and text messages are supported. Visual input features may be added in future versions.</p>"},{"location":"user/#7-can-i-list-places-i-want-to-visit-and-have-the-agent-plan-a-route","title":"7. Can I list places I want to visit and have the agent plan a route?","text":"<p>Yes! You can say: [Content appears to be incomplete in source document]</p>"},{"location":"user/#8-what-if-my-voice-message-had-a-mistake-can-i-correct-it","title":"8. What if my voice message had a mistake? Can I correct it?","text":"<p>No worries! You can simply say \"correct that\" or resend the message \u2014 the assistant will wait for your confirmation before moving on.</p>"},{"location":"user/#9-do-customers-need-to-install-anything","title":"9. Do customers need to install anything?","text":"<p>No installation required \u2014 everything runs inside WhatsApp. Just send a message and you're in.</p>"},{"location":"user/#10-how-many-languages-can-the-assistant-understand","title":"10. How many languages can the assistant understand?","text":"<p>The assistant can recognize multiple languages depending on the speech model (like Google STT or Whisper). It automatically detects the language based on your voice or text input.</p>"},{"location":"user/#whats-next","title":"What's Next?","text":"<p>We are currently in the process of integrating more advanced features and domains, more updates on the way\u2026</p>"},{"location":"user/#try-it-for-yourself","title":"Try it for yourself!","text":"<p>PatternAI Agent is here to make your daily tasks easier \u2014 your assistant listens, understands, and responds \u2014 just like a real conversation.</p> <p>Ready to START?</p> <p>\"Say it. Solve it.\"</p> <p></p>"},{"location":"whatsapp/","title":"PatternAI: WhatsApp Node","text":"<p>The WhatsApp Integration module extends the PatternAI Agent framework to support seamless, multi-modal conversational interactions through WhatsApp. It uses FastAPI to handle incoming webhook events from the Meta WhatsApp Business API, integrating tightly with the agent\u2019s LLM, STT, and TTS services.</p>"},{"location":"whatsapp/#system-architecture","title":"System Architecture","text":"<p>The module is structured for cloud-native deployment and includes:</p>"},{"location":"whatsapp/#development-environment","title":"Development Environment","text":"<ul> <li>Source Code: A FastAPI-based server to process WhatsApp messages via <code>/webhook</code>.</li> <li>Dockerfile: Defines the container definition for packaging the application.</li> <li>Configuration: Environment variables and Meta API credentials for secure operations.</li> </ul>"},{"location":"whatsapp/#deployment-workflow","title":"Deployment Workflow","text":"<ol> <li>Docker Build \u2013 Container image is created from the source.  </li> <li>Push to Registry \u2013 Stores the image in Google Container Registry.  </li> <li>Deploy \u2013 Creates or updates the Cloud Run service.</li> </ol>"},{"location":"whatsapp/#google-cloud-platform-infrastructure","title":"Google Cloud Platform Infrastructure","text":"<ul> <li> <p>Container Registry:   Stores Docker images at gcr. Provides versioned image storage for rollbacks. Integrates with Cloud Run for automatic deployments.</p> </li> <li> <p>Cloud Run:   Container runs the WhatsApp webhook service. Exposes <code>/webhook</code> endpoint for Meta events. Environment variables store Meta tokens and configuration.</p> </li> <li> <p>Cloud Logging:   Centralized logging for observability.</p> </li> </ul> <p></p>"},{"location":"whatsapp/#message-processing-flow","title":"Message Processing Flow","text":"<p>Incoming WhatsApp messages go through a structured pipeline:</p>"},{"location":"whatsapp/#components","title":"Components:","text":"<ul> <li><code>demo_whatsapp.py</code>: Entry point for handling FastAPI webhook logic.  </li> <li><code>whatsapp_base.py</code>: Base class that contains AI integration logic (LLM, STT, TTS).</li> </ul>"},{"location":"whatsapp/#sequence-of-operations","title":"Sequence of Operations","text":"Step Operation 1 User sends a message (text or audio) to the registered WhatsApp Business number. 2 Meta sends a webhook event with the message data via an HTTP POST request to <code>/webhook</code> on our FastAPI server. 3 Webhook Handler (<code>handle_incoming_message</code>) in WhatsApp class (from <code>demo_whatsapp.py</code>, inheriting <code>WhatsAppBase</code>) receives and logs the incoming data, ignoring non-user status updates. 4 Parse and Validate message using <code>parse_webhook_event()</code> from <code>WhatsAppBase</code>. 5 Deduplication check against Redis using <code>is_duplicate_message(message_id)</code>. If duplicate: <code>{\"status\": \"duplicate\", \"message_id\": message_id}</code>. If new: mark as processed. 6 Message Type Detection to route message to either <code>process_text_message</code> (for text) or <code>process_audio_message</code> (for voice)."},{"location":"whatsapp/#message-handling","title":"Message Handling","text":""},{"location":"whatsapp/#text-message-flow","title":"Text Message Flow","text":"<ol> <li><code>_call_llm()</code> processes the text via LLM.  </li> <li><code>_extract_message()</code> retrieves the LLM\u2019s reply.  </li> <li><code>_call_tts()</code> synthesizes the reply to audio.  </li> <li><code>send_audio_message()</code> delivers audio back to the user.</li> </ol>"},{"location":"whatsapp/#audio-message-flow","title":"Audio Message Flow","text":"<ol> <li><code>download_media()</code> retrieves the audio file.  </li> <li><code>_call_stt()</code> transcribes the audio.  </li> <li><code>_call_llm()</code> generates a response.  </li> <li><code>_call_tts()</code> converts it to audio.  </li> <li><code>send_audio_message()</code> returns the audio reply.</li> </ol>"},{"location":"whatsapp/#api-endpoint","title":"API Endpoint","text":"Endpoint Method Description <code>/webhook</code> POST Receives and processes WhatsApp events <p>On failure, a fallback message is sent: <code>Sorry, we couldn't process your request. Please try again later</code>. This ensures the system degrades gracefully under failure conditions.</p>"},{"location":"whatsapp/#logging-observability","title":"Logging &amp; Observability","text":"<ul> <li>Centralized logging via Cloud Logging   Tracks:</li> <li>Incoming webhook events  </li> <li>AI processing  </li> <li>Outgoing replies  </li> <li>Deduplication logic  </li> <li>Processing errors</li> </ul>"}]}