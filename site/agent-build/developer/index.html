<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=http://localhost:8000/agent-build/developer/ rel=canonical><link href=../ rel=prev><link href=../whatsapp/ rel=next><link rel=icon href=../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.16"><title>Developer Guide - PatternAI Labs Docs</title><link rel=stylesheet href=../../assets/stylesheets/main.7e37652d.min.css><link rel=stylesheet href=../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../styles/custom.css><script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=rgb(9,-12,-45) data-md-color-accent=rgb(9,-12,-45)> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#developer-insight-into-patternai-agent class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../.. title="PatternAI Labs Docs" class="md-header__button md-logo" aria-label="PatternAI Labs Docs" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> PatternAI Labs Docs </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Developer Guide </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=rgb(9,-12,-45) data-md-color-accent=rgb(9,-12,-45) aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=black aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../prerequisites/ class=md-tabs__link> Agent Build </a> </li> <li class=md-tabs__item> <a href=../../agent-test/setup/ class=md-tabs__link> Agent Test </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../.. title="PatternAI Labs Docs" class="md-nav__button md-logo" aria-label="PatternAI Labs Docs" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> PatternAI Labs Docs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Agent Build </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Agent Build </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../prerequisites/ class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=../setup/ class=md-nav__link> <span class=md-ellipsis> Installation Guide </span> </a> </li> <li class=md-nav__item> <a href=../ class=md-nav__link> <span class=md-ellipsis> Agent Build </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4 checked> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex> <span class=md-ellipsis> Support </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=true> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Support </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Developer Guide </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Developer Guide </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#overview class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> <nav class=md-nav aria-label=Overview> <ul class=md-nav__list> <li class=md-nav__item> <a href=#project-walkthrough-diving-into-the-system class=md-nav__link> <span class=md-ellipsis> Project Walkthrough: Diving into the System </span> </a> </li> <li class=md-nav__item> <a href=#the-foundation class=md-nav__link> <span class=md-ellipsis> The Foundation </span> </a> <nav class=md-nav aria-label="The Foundation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#llm-service-multi-agent-architecture class=md-nav__link> <span class=md-ellipsis> LLM Service: Multi-Agent Architecture </span> </a> <nav class=md-nav aria-label="LLM Service: Multi-Agent Architecture"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#tier-1-intent-recognition-tool-selection class=md-nav__link> <span class=md-ellipsis> Tier 1 : Intent Recognition &amp; Tool Selection </span> </a> </li> <li class=md-nav__item> <a href=#tier-2-contextual-conversation-management class=md-nav__link> <span class=md-ellipsis> Tier 2 : Contextual Conversation Management </span> </a> </li> <li class=md-nav__item> <a href=#tier-3-response-generation-built-in-validation class=md-nav__link> <span class=md-ellipsis> Tier 3 : Response Generation &amp; Built-in Validation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#service-traversal class=md-nav__link> <span class=md-ellipsis> Service Traversal </span> </a> <nav class=md-nav aria-label="Service Traversal"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#how-to-plug-in class=md-nav__link> <span class=md-ellipsis> How to plug in: </span> </a> </li> <li class=md-nav__item> <a href=#test-structure class=md-nav__link> <span class=md-ellipsis> Test Structure </span> </a> </li> <li class=md-nav__item> <a href=#refine-tune class=md-nav__link> <span class=md-ellipsis> Refine &amp; Tune </span> </a> <nav class=md-nav aria-label="Refine & Tune"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#design-philosophy class=md-nav__link> <span class=md-ellipsis> Design Philosophy </span> </a> </li> <li class=md-nav__item> <a href=#rule-engine class=md-nav__link> <span class=md-ellipsis> Rule Engine </span> </a> </li> <li class=md-nav__item> <a href=#filters-in-use class=md-nav__link> <span class=md-ellipsis> Filters in Use </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#versatility-and-modularity class=md-nav__link> <span class=md-ellipsis> Versatility and Modularity </span> </a> <nav class=md-nav aria-label="Versatility and Modularity"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#api-endpoints class=md-nav__link> <span class=md-ellipsis> API Endpoints </span> </a> </li> <li class=md-nav__item> <a href=#health-check class=md-nav__link> <span class=md-ellipsis> Health Check </span> </a> </li> <li class=md-nav__item> <a href=#1-speech-to-text-stt class=md-nav__link> <span class=md-ellipsis> 1. Speech-to-Text (STT) </span> </a> </li> <li class=md-nav__item> <a href=#2-language-model-llm-processing-sub-heading class=md-nav__link> <span class=md-ellipsis> 2. Language Model (LLM) Processing{: .sub-heading} </span> </a> </li> <li class=md-nav__item> <a href=#3-text-to-speech-tts class=md-nav__link> <span class=md-ellipsis> 3. Text-to-Speech (TTS) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#langfuse-for-prompt-management class=md-nav__link> <span class=md-ellipsis> Langfuse For Prompt Management </span> </a> <nav class=md-nav aria-label="Langfuse For Prompt Management"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#langfuse-for-tracking class=md-nav__link> <span class=md-ellipsis> Langfuse For Tracking </span> </a> </li> <li class=md-nav__item> <a href=#why-we-use-it-here class=md-nav__link> <span class=md-ellipsis> Why We Use It Here </span> </a> </li> <li class=md-nav__item> <a href=#langfuse-integration-usage class=md-nav__link> <span class=md-ellipsis> Langfuse Integration &amp; Usage </span> </a> </li> <li class=md-nav__item> <a href=#example-usage class=md-nav__link> <span class=md-ellipsis> Example Usage </span> </a> </li> <li class=md-nav__item> <a href=#service-specific-trackers class=md-nav__link> <span class=md-ellipsis> Service specific trackers </span> </a> <nav class=md-nav aria-label="Service specific trackers"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#metrics-tracked class=md-nav__link> <span class=md-ellipsis> Metrics Tracked: </span> </a> </li> <li class=md-nav__item> <a href=#metrics-tracked_1 class=md-nav__link> <span class=md-ellipsis> Metrics Tracked: </span> </a> </li> <li class=md-nav__item> <a href=#metrics-tracked_2 class=md-nav__link> <span class=md-ellipsis> Metrics Tracked: </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#whatsapp-integration class=md-nav__link> <span class=md-ellipsis> WhatsApp Integration </span> </a> <nav class=md-nav aria-label="WhatsApp Integration"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#next-steps class=md-nav__link> <span class=md-ellipsis> Next Steps </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#resources class=md-nav__link> <span class=md-ellipsis> Resources </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../whatsapp/ class=md-nav__link> <span class=md-ellipsis> Integration </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex=0> <span class=md-ellipsis> Agent Test </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Agent Test </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../agent-test/setup/ class=md-nav__link> <span class=md-ellipsis> Installation Guide </span> </a> </li> <li class=md-nav__item> <a href=../../agent-test/ class=md-nav__link> <span class=md-ellipsis> Agent Test </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#overview class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> <nav class=md-nav aria-label=Overview> <ul class=md-nav__list> <li class=md-nav__item> <a href=#project-walkthrough-diving-into-the-system class=md-nav__link> <span class=md-ellipsis> Project Walkthrough: Diving into the System </span> </a> </li> <li class=md-nav__item> <a href=#the-foundation class=md-nav__link> <span class=md-ellipsis> The Foundation </span> </a> <nav class=md-nav aria-label="The Foundation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#llm-service-multi-agent-architecture class=md-nav__link> <span class=md-ellipsis> LLM Service: Multi-Agent Architecture </span> </a> <nav class=md-nav aria-label="LLM Service: Multi-Agent Architecture"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#tier-1-intent-recognition-tool-selection class=md-nav__link> <span class=md-ellipsis> Tier 1 : Intent Recognition &amp; Tool Selection </span> </a> </li> <li class=md-nav__item> <a href=#tier-2-contextual-conversation-management class=md-nav__link> <span class=md-ellipsis> Tier 2 : Contextual Conversation Management </span> </a> </li> <li class=md-nav__item> <a href=#tier-3-response-generation-built-in-validation class=md-nav__link> <span class=md-ellipsis> Tier 3 : Response Generation &amp; Built-in Validation </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#service-traversal class=md-nav__link> <span class=md-ellipsis> Service Traversal </span> </a> <nav class=md-nav aria-label="Service Traversal"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#how-to-plug-in class=md-nav__link> <span class=md-ellipsis> How to plug in: </span> </a> </li> <li class=md-nav__item> <a href=#test-structure class=md-nav__link> <span class=md-ellipsis> Test Structure </span> </a> </li> <li class=md-nav__item> <a href=#refine-tune class=md-nav__link> <span class=md-ellipsis> Refine &amp; Tune </span> </a> <nav class=md-nav aria-label="Refine & Tune"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#design-philosophy class=md-nav__link> <span class=md-ellipsis> Design Philosophy </span> </a> </li> <li class=md-nav__item> <a href=#rule-engine class=md-nav__link> <span class=md-ellipsis> Rule Engine </span> </a> </li> <li class=md-nav__item> <a href=#filters-in-use class=md-nav__link> <span class=md-ellipsis> Filters in Use </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#versatility-and-modularity class=md-nav__link> <span class=md-ellipsis> Versatility and Modularity </span> </a> <nav class=md-nav aria-label="Versatility and Modularity"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#api-endpoints class=md-nav__link> <span class=md-ellipsis> API Endpoints </span> </a> </li> <li class=md-nav__item> <a href=#health-check class=md-nav__link> <span class=md-ellipsis> Health Check </span> </a> </li> <li class=md-nav__item> <a href=#1-speech-to-text-stt class=md-nav__link> <span class=md-ellipsis> 1. Speech-to-Text (STT) </span> </a> </li> <li class=md-nav__item> <a href=#2-language-model-llm-processing-sub-heading class=md-nav__link> <span class=md-ellipsis> 2. Language Model (LLM) Processing{: .sub-heading} </span> </a> </li> <li class=md-nav__item> <a href=#3-text-to-speech-tts class=md-nav__link> <span class=md-ellipsis> 3. Text-to-Speech (TTS) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#langfuse-for-prompt-management class=md-nav__link> <span class=md-ellipsis> Langfuse For Prompt Management </span> </a> <nav class=md-nav aria-label="Langfuse For Prompt Management"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#langfuse-for-tracking class=md-nav__link> <span class=md-ellipsis> Langfuse For Tracking </span> </a> </li> <li class=md-nav__item> <a href=#why-we-use-it-here class=md-nav__link> <span class=md-ellipsis> Why We Use It Here </span> </a> </li> <li class=md-nav__item> <a href=#langfuse-integration-usage class=md-nav__link> <span class=md-ellipsis> Langfuse Integration &amp; Usage </span> </a> </li> <li class=md-nav__item> <a href=#example-usage class=md-nav__link> <span class=md-ellipsis> Example Usage </span> </a> </li> <li class=md-nav__item> <a href=#service-specific-trackers class=md-nav__link> <span class=md-ellipsis> Service specific trackers </span> </a> <nav class=md-nav aria-label="Service specific trackers"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#metrics-tracked class=md-nav__link> <span class=md-ellipsis> Metrics Tracked: </span> </a> </li> <li class=md-nav__item> <a href=#metrics-tracked_1 class=md-nav__link> <span class=md-ellipsis> Metrics Tracked: </span> </a> </li> <li class=md-nav__item> <a href=#metrics-tracked_2 class=md-nav__link> <span class=md-ellipsis> Metrics Tracked: </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#whatsapp-integration class=md-nav__link> <span class=md-ellipsis> WhatsApp Integration </span> </a> <nav class=md-nav aria-label="WhatsApp Integration"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#next-steps class=md-nav__link> <span class=md-ellipsis> Next Steps </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#resources class=md-nav__link> <span class=md-ellipsis> Resources </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 class=title id=developer-insight-into-patternai-agent>Developer Insight into PatternAI Agent</h1> <h2 class=sub-heading id=overview>Overview</h2> <p>PatternAI Agent is a modular, voice-enabled conversational agent framework built to deliver seamless, human-like interactions via audio and text channels through WhatsApp.</p> <p><img alt=Overview class=projected-img src=../images/diag1.png></p> <p>The system integrates multiple Speech-to-Text (STT) and Text-to-Speech (TTS) services, connects to powerful Large Language Models (LLMs) through OpenRouter, and leverages Langfuse for prompt tracing and monitoring. It's designed with a clean backend and modular agent profiles for domain-specific use cases like shopping, travel assistance, etc.</p> <h3 class=sub-heading id=project-walkthrough-diving-into-the-system>Project Walkthrough: Diving into the System</h3> <p>To understand the operation of the PatternAI Agent, this section will guide through the process flow, from the initial receipt of a user's request to the generation and delivery of a response. Key system components and their interactions will be unpacked.</p> <h3 class=sub-heading id=the-foundation>The Foundation</h3> <ul> <li><strong><a href=# style=color:rgb(7,10,39);>/app</a></strong> : This directory houses the core application logic and services that drive the agent's functionality. It includes:</li> </ul> <p><strong><a href=# style=color:rgb(7,10,39);>/services</a></strong> : encapsulates the core AI processing logic of the PatternAI Agent. It defines abstract interfaces and provides concrete implementations for key AI processing capabilities.</p> <p>These capabilities are organized into the following sub-modules:</p> <details class=note open=open> <summary>services/</summary> <div class=highlight><pre><span></span><code>services/
├── llm/
│   ├── __init__.py                # Factory: creates LLM handler (OpenRouter/custom)
│   ├── base_handler.py            # Abstract base class for LLMs (defines prompt + interface)
│   ├── openrouter_handler.py      # Implementation using OpenRouter API
│   ├── router_handler.py          # Routes queries to tools based on user intent
│   ├── conversation_history.py    # Maintains chat history with truncation logic
│   ├── prompts/                   # Prompt management logic (Langfuse or static)
│   ├── tools/                     # Tool execution logic and OpenAI-compatible tool specs
│   └── validators/                # Streaming and final response validation logic
│
├── stt/
│   ├── __init__.py                # Factory: selects STT engine
│   ├── base_stt_handler.py        # Abstract base class for STT services
│   ├── whisper_stt.py             # STT via OpenAI Whisper API
│   ├── google_stt.py              # STT via Google Cloud Speech API
│   ├── elevenlabs_stt.py          # STT via ElevenLabs API
│   └── stt_validation.py          # Validation and metrics for STT transcription
│
└── tts/
    ├── __init__.py               # Factory: selects TTS engine
    ├── base_tts_handler.py       # Abstract TTS base class, defines text-to-audio interface
    ├── edge_tts.py               # TTS implementation using Microsoft Edge TTS
    └── tts_validation.py         # Validation for text-to-speech output 
</code></pre></div> </details> <p><strong><a href=# style=color:rgb(7,10,39);>/llm</a></strong> : orchestrates intelligent conversation flow through a sophisticated multi-layered architecture that transforms user intent into contextual, validated responses.Rather than simply connecting to language models, this module implements a complete reasoning pipeline that routes requests, maintains conversation context, validates outputs, and ensures reliable agent behavior.</p> <h4 class=sub-heading id=llm-service-multi-agent-architecture>LLM Service: Multi-Agent Architecture</h4> <h5 id=tier-1-intent-recognition-tool-selection><strong>Tier 1 : Intent Recognition &amp; Tool Selection</strong></h5> <p>How does the system know where to send a request like "I need donuts"?</p> <p>It’s handled by <code>router_handler.py/</code> and <code>tools.py/</code> using a dedicated LLM as an intelligent dispatcher. Using the configured LLM, it analyzes user intent and selects the most appropriate specialized tool from the available arsenal defined in tools_config.yaml. This isn't simple keyword matching - the router understands context, recognizing the difference between "I need coffee" <strong>(shopping intent)</strong> and "I want to travel to coffee county" <strong>(travel intent)</strong>. The routing system leverages a structured approach:</p> <ul> <li>Analyzes user input against tool descriptions from <strong>ROUTER_SYSTEM_PROMPT</strong></li> <li>returns JSON-formatted routing decisions via <code>RouterHandler.route()</code></li> <li><code>initialize_tools()</code> creates bidirectional mappings between tools and agents</li> <li>Tools operate through unified interface <code>base_tool.py</code> with consistent behavior</li> <li>Each tool knows its agent ID, enabling seamless transition to specialized processing</li> </ul> <div class="admonition note"> <p class=admonition-title>Note</p> <p>New domains can be added by simply creating new tool classes and updating the YAML configuration</p> </div> <h5 id=tier-2-contextual-conversation-management><strong>Tier 2 : Contextual Conversation Management</strong></h5> <p>Before any response generation begins, the layer <code>conversation_history.py/</code> enriches the interaction with memory. The ConversationHistory class maintains an intelligent sliding window of past interactions, applying both message count limits and token-based truncation to optimize context while preserving memory efficiency.</p> <h5 id=tier-3-response-generation-built-in-validation><strong>Tier 3 : Response Generation &amp; Built-in Validation</strong></h5> <p>This layer orchestrates LLM interaction through OpenRouterHandler, serving as the bridge between the internal agent system and external language models. This handler manages the complete response lifecycle: This handler manages the complete response lifecycle with integrated prompt management, streaming generation, real-time Pydantic validation of all streaming LLM responses to ensure structured output formatting and type safety.</p> <p><strong><a href=# style=color:rgb(7,10,39);>/stt</a></strong> : a unified interface for converting speech to text using multiple cloud providers. It supports Whisper (OpenAI), Google Cloud Speech-to-Text, and ElevenLabs services through a common abstraction layer.</p> <p><strong><a href=# style=color:rgb(7,10,39);>/tts</a></strong> : a standardized interface for converting text to speech using various synthesis engines. Currently implemented with Microsoft Edge's TTS service, the module is designed for easy extensibility to support additional providers.</p> <h3 class=sub-heading id=service-traversal>Service Traversal</h3> <p>The system operates through a series of coordinated steps that leverage the abstract interfaces and factory pattern to process user requests. <img alt class=projected-img src=../images/diag2.png></p> <p>When a client interacts with the system, the request follows a consistent architectural pattern across all service types (STT, LLM, and TTS):</p> <p>While processing an audio file, the system uses factory functions like <code>create_stt_service</code> to instantiate the appropriate service (Whisper, Google STT, or ElevenLabs). These factories handle configuration parsing, environment variable injection, and service initialization in one centralized location.</p> <p>After initialization, the system interacts with each service through its abstract interface. For example, calling <code>transcribe()</code> on any STT service produces a text transcript regardless of which underlying provider (Whisper, Google, or ElevenLabs) is actually processing the audio.</p> <p>The LLM processing and text-to-speech (TTS) flows follow a similar architectural pattern.</p> <ul> <li>For LLMs, the system uses create_llm_handler to instantiate an OpenRouterHandler, which acts as the gateway to OpenRouter-backed language models. It supports prompt injection, conversation context, real-time streaming, and structured response validation, as detailed in the LLM service section above.</li> <li>For TTS, the <code>create_tts_service</code> factory function initializes an EdgeTTS service that converts text input into audio bytes using the Microsoft Edge TTS engine.</li> </ul> <p>While models provide services by default, their performance can vary, a topic we'll explore using Langfuse later. Keeping that in mind, we've built a framework that allows developers to integrate models as they please.</p> <h4 class=sub-heading id=how-to-plug-in>How to plug in:</h4> <p>For instance, let's integrate a new STT service:</p> <ul> <li>Implement a handler class inheriting from <code>BaseSTTClass</code>.</li> <li>Add it to the <code>service_registry</code> in <code>create_stt_service</code>.</li> </ul> <div class=highlight><pre><span></span><code><span class=n>service_registry</span> <span class=o>=</span> <span class=p>{</span>
    <span class=s2>&quot;whisper&quot;</span><span class=p>:</span> <span class=n>WhisperSTT</span><span class=p>,</span>
    <span class=s2>&quot;google&quot;</span><span class=p>:</span> <span class=n>GoogleSTT</span><span class=p>,</span>
    <span class=s2>&quot;elevenlabs&quot;</span><span class=p>:</span> <span class=n>ElevenLabsSTT</span><span class=p>,</span>
    <span class=s2>&quot;new_service&quot;</span><span class=p>:</span> <span class=n>NewSTTService</span>  <span class=c1># add here</span>
<span class=p>}</span>
</code></pre></div> <p>Also we've optimized performance by implementing factory functions with LRU caching and custom cache logging decorators, preventing redundant service creation when the same configuration is requested multiple times. This allows dynamic, efficient, and configurable creation of service instances at runtime.</p> <p><strong><a href=# style=color:rgb(7,10,39);>/tests</a></strong> : This directory contains unit and integration tests for the application logic, ensuring the reliability and correctness of the core functionalities. Tests are executed using the pytest framework.</p> <p>To run all tests, use the following command:</p> <div class=highlight><pre><span></span><code>poetry<span class=w> </span>run<span class=w> </span>pytest
</code></pre></div> <h4 class=sub-heading id=test-structure>Test Structure</h4> <p>The <strong><a href=# style=color:rgb(7,10,39);>/tests</a></strong> directory is organized by module, with each file typically containing tests for a specific part of the application:</p> <ul> <li><code>test_llm.py</code>: Contains tests for the Large Language Model (LLM) integration, including schema validation and semantic comparison of LLM outputs</li> <li><code>test_stt.py</code>: Contains tests for the Speech-to-Text (STT) service, covering different STT providers and transcription functionality</li> <li><code>test_tts.py</code>: Contains tests for the Text-to-Speech (TTS) service, including service creation, configuration, and speech synthesis</li> <li><code>conftest.py</code>: Provides helper functions and fixtures used across multiple test files. For example, the <code>cleaned_json</code> function normalizes JSON outputs from LLMs, which is crucial for testing LLM responses</li> </ul> <h4 class=sub-heading id=refine-tune>Refine &amp; Tune</h4> <p>Creating safer, more humane conversational agents meant taking on the responsibility of ensuring responses remain fair, respectful, and trustworthy — not just intelligent.</p> <p>With that in mind, <strong><a href=# style=color:rgb(7,10,39);>/filters</a></strong> : Provides a modular text filtering framework that enforces ethical and contextual guardrails for agent outputs. It is responsible for detecting and handling: </p> <ul> <li>Personally Identifiable Information (PII) </li> <li>Biased language </li> <li>Profanity </li> </ul> <p>Every filter is pluggable and configurable, allowing easy customization per agent use case.</p> <h5 class=sub-heading id=design-philosophy>Design Philosophy</h5> <p>Filters follow the Strategy Pattern, built on a shared <code>BaseFilter</code> interface. Each filter implements an <code>apply(text: str) -&gt; str</code> method, ensuring consistency and interchangeability.</p> <div class=highlight><pre><span></span><code><span class=k>class</span><span class=w> </span><span class=nc>BaseFilter</span><span class=p>(</span><span class=n>ABC</span><span class=p>):</span>
    <span class=nd>@abstractmethod</span>
    <span class=k>def</span><span class=w> </span><span class=nf>apply</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>str</span><span class=p>:</span>
        <span class=err>…</span>
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Note</p> <p>This design allows us to dynamically register and apply filters, configured declaratively through a YAML file.</p> </div> <h5 class=sub-heading id=rule-engine>Rule Engine</h5> <p>At runtime, the RuleEngine loads filters defined in configs.yaml, instantiates them, and applies them sequentially to incoming text enabling centralized control, easy reordering or disabling of filters and detailed logging of filter application.</p> <h5 id=filters-in-use>Filters in Use</h5> <ul> <li>BiasFilter – Replaces predefined biased terms with neutral ones using regex-based replacements.</li> <li>PIIFilter – Detects and anonymizes PII in plain text and json using presidio, with field-specific handling for dates and locations.</li> <li>ProfanityFilter – Censors offensive language using both regex and the better_profanity library.</li> </ul> <h3 class=sub-heading id=versatility-and-modularity>Versatility and Modularity</h3> <ul> <li><strong><a href=# style=color:rgb(7,10,39);>/clients</a></strong> : provides a flexible framework for implementing domain-specific conversational agents with WhatsApp integration. Each client gets its own isolated configuration and implementation while sharing core AI services (STT/TTS/LLM).</li> </ul> <h4 class=sub-heading id=api-endpoints>API Endpoints</h4> <p>With the core services for STT, TTS, and LLM dynamically monitored through our modular framework, these capabilities are made accessible to client applications via a set of clean, RESTful API endpoints. These endpoints handle everything from speech transcription to language and audio generation, providing a unified interface for seamless multi-modal conversations.</p> <p>The service exposes the following endpoints:</p> <table> <thead> <tr> <th>Endpoint</th> <th>Method</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><code>/health</code></td> <td>GET</td> <td>Health check endpoint</td> </tr> <tr> <td><code>/stt</code></td> <td>POST</td> <td>Speech-to-Text transcription</td> </tr> <tr> <td><code>/llm</code></td> <td>POST</td> <td>Process text with LLM models</td> </tr> <tr> <td><code>/tts</code></td> <td>POST</td> <td>Text-to-Speech synthesis</td> </tr> </tbody> </table> <h4 class=sub-heading id=health-check>Health Check</h4> <p>Verify if the service is running:</p> <div class=highlight><pre><span></span><code>curl<span class=w> </span>-X<span class=w> </span>GET<span class=w> </span><span class=s2>&quot;http://localhost:5000/health&quot;</span>
</code></pre></div> <p><strong>Response:</strong> <div class=highlight><pre><span></span><code><span class=p>{</span><span class=nt>&quot;status&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;ok&quot;</span><span class=p>}</span>
</code></pre></div></p> <h4 class=sub-heading id=1-speech-to-text-stt>1. Speech-to-Text (STT)</h4> <p>Convert audio to text using the <code>/stt</code> endpoint.</p> <p><strong>Request</strong></p> <div class=highlight><pre><span></span><code>curl<span class=w> </span>-X<span class=w> </span>POST<span class=w> </span><span class=s2>&quot;http://localhost:5000/stt&quot;</span><span class=w> </span><span class=se>\</span>
<span class=w>  </span>-H<span class=w> </span><span class=s2>&quot;Content-Type: application/octet-stream&quot;</span><span class=w> </span><span class=se>\</span>
<span class=w>  </span>-H<span class=w> </span><span class=s2>&quot;STT-Model: whisper&quot;</span><span class=w> </span><span class=se>\</span>
<span class=w>  </span>--data-binary<span class=w> </span>@<span class=s2>&quot;audio_file.webm&quot;</span>
</code></pre></div> <p><strong>Available STT models:</strong> - <code>whisper</code> (OpenAI's Whisper model) - <code>google</code> (Google Speech-to-Text) - <code>elevenlabs</code> (ElevenLabs STT service)</p> <p>The model can be selected via the <code>STT-Model</code> header. If not specified, the default model configured in <code>configs.yaml</code> will be used.</p> <p><strong>Response:</strong> <div class=highlight><pre><span></span><code><span class=p>{</span>
<span class=w>  </span><span class=nt>&quot;status&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;success&quot;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;transcription&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;This is the transcribed text from the audio.&quot;</span><span class=p>,</span>
<span class=w>  </span><span class=nt>&quot;request_id&quot;</span><span class=p>:</span><span class=w> </span><span class=s2>&quot;unique-request-id&quot;</span>
<span class=p>}</span>
</code></pre></div></p> <h4 id=2-language-model-llm-processing-sub-heading>2. Language Model (LLM) Processing{: .sub-heading}</h4> <p>Process transcribed text with an LLM model using the <code>/llm</code> endpoint.</p> <p><strong>Request</strong></p> <div class=highlight><pre><span></span><code>curl<span class=w> </span>-X<span class=w> </span>POST<span class=w> </span><span class=s2>&quot;http://localhost:5000/llm&quot;</span><span class=w> </span><span class=se>\</span>
<span class=w>  </span>-H<span class=w> </span><span class=s2>&quot;Content-Type: application/json&quot;</span><span class=w> </span><span class=se>\</span>
<span class=w>  </span>-d<span class=w> </span><span class=s1>&#39;{</span>
<span class=s1>    &quot;transcription&quot;: &quot;What items do you have on sale?&quot;,</span>
<span class=s1>    &quot;model_name&quot;: &quot;anthropic/claude-3-haiku-20240307&quot;,</span>
<span class=s1>    &quot;agent_id&quot;: &quot;agent_shopping&quot;</span>
<span class=s1>  }&#39;</span>
</code></pre></div> <p><strong>Notes:</strong> - <code>model_name</code>: Specifies which LLM model to use (required if <code>fallback_enabled</code> is set to false in configuration) - <code>agent_id</code>: Optional parameter to specify which agent profile to use (e.g., "agent_shopping", "agent_travel")</p> <p><strong>Response:</strong></p> <p>The response is streamed in plain text format:</p> <div class=highlight><pre><span></span><code>request_id: unique-request-id

We have several items on sale this week. Our fresh produce section has discounts on seasonal fruits and vegetables. In the grocery aisle, we have deals on pasta, rice, and canned goods. The bakery section has a buy-one-get-one offer on whole wheat bread. Is there anything specific you&#39;re looking for?
</code></pre></div> <h4 class=sub-heading id=3-text-to-speech-tts>3. Text-to-Speech (TTS)</h4> <p>Convert text to speech using the <code>/tts</code> endpoint.</p> <p><strong>Request</strong></p> <div class=highlight><pre><span></span><code>curl<span class=w> </span>-X<span class=w> </span>POST<span class=w> </span><span class=s2>&quot;http://localhost:5000/tts&quot;</span><span class=w> </span><span class=se>\</span>
<span class=w>  </span>-H<span class=w> </span><span class=s2>&quot;Content-Type: application/json&quot;</span><span class=w> </span><span class=se>\</span>
<span class=w>  </span>-H<span class=w> </span><span class=s2>&quot;TTS-Model: edge_tts&quot;</span><span class=w> </span><span class=se>\</span>
<span class=w>  </span>-d<span class=w> </span><span class=s1>&#39;{</span>
<span class=s1>    &quot;message&quot;: &quot;Hello, we have several items on sale this week. Is there anything specific you&#39;</span><span class=se>\&#39;</span><span class=s1>&#39;re looking for?&quot;</span>
<span class=s1>  }&#39;</span><span class=w> </span><span class=se>\</span>
<span class=w>  </span>--output<span class=w> </span>speech.mp3
</code></pre></div> <p><strong>Available TTS models:</strong> - <code>edge_tts</code> (Microsoft Edge TTS)</p> <p><strong>Response:</strong></p> <p>The response is an audio file in MP3 format with appropriate headers:</p> <div class=highlight><pre><span></span><code>Content-Type: audio/mpeg
Content-Disposition: attachment; filename=speech.mp3
X-Request-ID: unique-request-id
</code></pre></div> <h3 class=sub-heading id=langfuse-for-prompt-management>Langfuse For Prompt Management</h3> <blockquote> <p>"For a transparent view into the agent's language-driven interactions and a pathway to iterative improvement, the framework leverages Langfuse for robust prompt management and comprehensive monitoring."</p> </blockquote> <p>To ensure that PatternAI Agent remains highly modular and adaptable, prompt management is architected with a clear separation of concerns.</p> <p>We've established an abstract <strong>Base Interface</strong> (<code>BasePromptManager</code>) - defined in <code>/base_prompt_handler.py</code>, which serves as an abstract interface for prompt retrieval. This allows us to seamlessly integrate various prompt management solutions without impacting the core logic that interacts with LLMs.</p> <p>Our current implementation <strong>LangfusePromptManager</strong> - defined in <code>/langfuse_handler.py</code>, leverages the Langfuse SDK to dynamically fetch prompt definitions from a centralized prompt store. This approach offers several key advantages:</p> <ul> <li><strong>Centralized Prompt Management</strong>: All prompt templates are managed within Langfuse, providing a single source of truth and simplifying updates.</li> <li><strong>Dynamic Updates</strong>: Changes to prompts in Langfuse are reflected in the agent without requiring code deployments.</li> <li><strong>Version Control &amp; History</strong>: Langfuse provides built-in versioning and history tracking for prompts, crucial for experimentation and rollback.</li> <li><strong>Observability</strong>: Langfuse integrates seamlessly with prompt tracing and monitoring, offering valuable insights into prompt performance and usage.</li> </ul> <p><br></p> <p><img alt class=projected-img src=../images/diag3.png></p> <p><br></p> <p>The <code>PromptManager</code> interface dictates that any concrete implementation must provide a <code>get_prompt(name)</code> method. Our <code>LangfusePromptManager</code> fulfills this contract by initializing the Langfuse SDK using environment variables (<code>LANGFUSE_PUBLIC_KEY</code>, <code>LANGFUSE_SECRET_KEY</code>, <code>LANGFUSE_HOST</code>) and retrieving prompts by their unique name using the Langfuse client:</p> <div class=highlight><pre><span></span><code><span class=bp>self</span><span class=o>.</span><span class=n>client</span><span class=o>.</span><span class=n>get_prompt</span><span class=p>(</span><span class=n>name</span><span class=p>)</span>
</code></pre></div> <p>This decoupling empowers us to evolve our prompt management strategy in the future – simply by implementing a new class that adheres to the <code>PromptManager</code> interface. This design ensures that the PatternAI Agent remains flexible and maintainable as our requirements evolve.</p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>Want to customize or extend the monitoring? <br> You can easily plug in your own prompt management or analytics solution too. To learn more about Langfuse, visit their official documentation (<a href=https://langfuse.com/docs>https://langfuse.com/docs</a>).</p> </div> <h4 class=sub-heading id=langfuse-for-tracking>Langfuse For Tracking</h4> <p>To gain precise observability into the performance and behavior of our multi-modal services – including language modeling (LLM), speech-to-text (STT), and text-to-speech (TTS) – we integrate Langfuse. This provides structured, end-to-end tracing of interactions across services, enabling detailed logging of input/output data, metadata, cost, and latency for each API call in our application pipeline.</p> <h4 class=sub-heading id=why-we-use-it-here>Why We Use It Here</h4> <p>In multi-modal AI architectures, especially those combining LLM, STT, and TTS services, monitoring individual service calls and aggregating their performance and cost metrics can be complex. Langfuse simplifies this by:</p> <ul> <li>Enabling real-time cost tracking per API call.</li> <li>Capturing performance metrics such as latency, token usage, audio duration, and processing time.</li> <li>Providing unified tracing across services, allowing correlation of input/output pairs and their associated metadata.</li> <li>Supporting prompt versioning and retrieval analytics for LLM generations.</li> </ul> <h4 class=sub-heading id=langfuse-integration-usage>Langfuse Integration &amp; Usage</h4> <p>To enable request tracing and detailed observability across all AI components, Langfuse is initialized using credentials securely stored in environment variables.</p> <p>Langfuse's SDK is initialized in each tracker with:</p> <div class=highlight><pre><span></span><code><span class=bp>self</span><span class=o>.</span><span class=n>langfuse</span> <span class=o>=</span> <span class=n>Langfuse</span><span class=p>(</span>
    <span class=n>public_key</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&quot;LANGFUSE_PUBLIC_KEY&quot;</span><span class=p>),</span>
    <span class=n>secret_key</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&quot;LANGFUSE_SECRET_KEY&quot;</span><span class=p>),</span>
    <span class=n>host</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&quot;LANGFUSE_HOST&quot;</span><span class=p>)</span>
<span class=p>)</span>
</code></pre></div> <p>This Langfuse instance is then used throughout the application to track operations and metrics for each service.</p> <h4 id=example-usage>Example Usage</h4> <p>Langfuse tracing relies primarily on two methods:</p> <ul> <li><code>trace()</code> is used to start a new tracing session, encapsulating the full flow of a request.</li> <li><code>generation()</code> records specific input/output events, such as transcriptions, responses, or synthesized speech, including performance and cost metadata.</li> </ul> <p><strong>Example: Starting a trace</strong></p> <div class=highlight><pre><span></span><code><span class=n>trace</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>langfuse</span><span class=o>.</span><span class=n>trace</span><span class=p>(</span>
    <span class=n>name</span><span class=o>=</span><span class=s2>&quot;cost_analysis&quot;</span><span class=p>,</span>
    <span class=n>user_id</span><span class=o>=</span><span class=n>user_id</span><span class=p>,</span>
    <span class=n>metadata</span><span class=o>=</span><span class=p>{</span><span class=s2>&quot;environment&quot;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>getenv</span><span class=p>(</span><span class=s2>&quot;ENV&quot;</span><span class=p>,</span> <span class=s2>&quot;dev&quot;</span><span class=p>),</span> <span class=s2>&quot;source&quot;</span><span class=p>:</span> <span class=s2>&quot;speech_input&quot;</span><span class=p>}</span>
<span class=p>)</span>
</code></pre></div> <p><strong>Example: Logging a generation event</strong></p> <div class=highlight><pre><span></span><code><span class=n>trace</span><span class=o>.</span><span class=n>generation</span><span class=p>(</span>
    <span class=n>name</span><span class=o>=</span><span class=sa>f</span><span class=s2>&quot;stt-transcription-</span><span class=si>{</span><span class=n>model_name</span><span class=si>}</span><span class=s2>&quot;</span><span class=p>,</span>
    <span class=nb>input</span><span class=o>=</span><span class=sa>f</span><span class=s2>&quot;[Audio duration: </span><span class=si>{</span><span class=n>audio_duration_sec</span><span class=si>:</span><span class=s2>.2f</span><span class=si>}</span><span class=s2>s]&quot;</span><span class=p>,</span>
    <span class=n>output</span><span class=o>=</span><span class=n>transcription</span><span class=p>,</span>
    <span class=n>model</span><span class=o>=</span><span class=n>model_name</span><span class=p>,</span>
    <span class=n>metadata</span><span class=o>=</span><span class=p>{</span>
        <span class=s2>&quot;duration_audio&quot;</span><span class=p>:</span> <span class=nb>round</span><span class=p>(</span><span class=n>audio_duration_sec</span><span class=p>,</span> <span class=mi>8</span><span class=p>),</span>
        <span class=s2>&quot;cost&quot;</span><span class=p>:</span> <span class=nb>round</span><span class=p>(</span><span class=n>cost</span><span class=p>,</span> <span class=mi>8</span><span class=p>),</span>
        <span class=s2>&quot;currency&quot;</span><span class=p>:</span> <span class=s2>&quot;USD&quot;</span><span class=p>,</span>
    <span class=p>}</span>
<span class=p>)</span>
</code></pre></div> <h4 class=sub-heading id=service-specific-trackers>Service specific trackers</h4> <p>We encapsulate Langfuse functionality within dedicated tracker classes for each core service.</p> <p><strong>LLMTracker</strong> → <code>/clients/demo/llm_tracker.py</code></p> <p>Handles all tracing related to LLM interactions. It initializes with model-specific configuration such as token pricing and supported providers from <code>configs.yaml</code>, starts a trace with <code>start_trace()</code>, and records generation event with Langfuse using <code>track_llm()</code>.</p> <h6 id=metrics-tracked>Metrics Tracked:</h6> <ul> <li>Input/output tokens</li> <li>Latency and response time</li> <li>Cost per model/token type</li> <li>Prompt metadata</li> </ul> <p><strong>STTTracker</strong> → <code>/clients/demo/stt_tracker.py</code></p> <p>Manages speech-to-text tracing. It loads model and pricing info, starts a trace via <code>start_trace()</code>, and logs the transcription with <code>track_stt()</code>.</p> <h6 id=metrics-tracked_1>Metrics Tracked:</h6> <ul> <li>Audio duration</li> <li>Transcription text</li> <li>STT model used</li> <li>Cost per second</li> </ul> <p><strong>TTSTracker</strong> → <code>/clients/demo/tts_tracker.py</code></p> <p>Tracks text-to-speech operations. At initialization, it reads the default voice model and pricing scheme—covering both per-character and per-second billing—from the config, starts a trace with <code>start_trace()</code>, and logs synthesis details in <code>track_tts()</code>.</p> <h6 id=metrics-tracked_2>Metrics Tracked:</h6> <ul> <li>Character count</li> <li>Audio duration</li> <li>Processing time</li> <li>Model and voice used</li> <li>Combined cost (char + audio-based)</li> </ul> <p>All configuration values — such as pricing, model names, and default services — are centrally managed via <code>configs.yaml</code>.</p> <p>After each conversation, the metrics are calculated and visualized in Langfuse's dashboard, where teams monitor cost-per-session or drill into LLM token usage by model. <br></p> <p><img alt="Langfuse Dashboard" class=projected-img src=../images/lang.jpeg></p> <h3 class=sub-heading id=whatsapp-integration>WhatsApp Integration</h3> <p>Support for WhatsApp messaging is currently being developed to enable seamless voice and text interactions through the platform. Hop into Whatsapp <a href=../whatsapp/ >Integration.</a></p> <hr> <p>This document has provided a comprehensive overview of the PatternAI Agent, a modular framework for building voice-enabled conversational agents. We covered the system's architecture, key components, and how they interact to process user requests. The agent's flexibility is enabled by its modular design, support for multiple STT, TTS, and LLM providers, and the use of Langfuse for detailed observability.</p> <h4 class=sub-heading id=next-steps>Next Steps</h4> <p>To get started with the PatternAI Agent:</p> <ol> <li>Ensure you have met all the system requirements.</li> <li>Follow the instructions in the <a href=../setup/ >Installation Guide</a> to set up the environment and dependencies.</li> <li>Explore the example clients in the <code>clients/</code> directory.</li> </ol> <h3 class=sub-heading id=resources>Resources</h3> <ul> <li><strong>GitHub Repository</strong>: <a href=https://github.com/PatternAILabs/patternai-agent-build.git>PatternAI Agent</a></li> </ul> <p>We encourage you to leverage the PatternAI Agent to create innovative and engaging conversational experiences. We welcome your feedback and contributions!</p> <p><br></p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../..", "features": ["navigation.tabs", "navigation.sections"], "search": "../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../assets/javascripts/bundle.50899def.min.js></script> </body> </html>