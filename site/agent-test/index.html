<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=http://localhost:8000/agent-test/ rel=canonical><link href=setup/ rel=prev><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.16"><title>Agent Test - PatternAI Labs Docs</title><link rel=stylesheet href=../assets/stylesheets/main.7e37652d.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../styles/custom.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=rgb(9,-12,-45) data-md-color-accent=rgb(9,-12,-45)> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#agenttest-from-prompt-to-performance class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="PatternAI Labs Docs" class="md-header__button md-logo" aria-label="PatternAI Labs Docs" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> PatternAI Labs Docs </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Agent Test </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=rgb(9,-12,-45) data-md-color-accent=rgb(9,-12,-45) aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=black aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> Home </a> </li> <li class=md-tabs__item> <a href=../agent-build/prerequisites/ class=md-tabs__link> Agent Build </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=setup/ class=md-tabs__link> Agent Test </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="PatternAI Labs Docs" class="md-nav__button md-logo" aria-label="PatternAI Labs Docs" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> PatternAI Labs Docs </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex=0> <span class=md-ellipsis> Agent Build </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Agent Build </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../agent-build/prerequisites/ class=md-nav__link> <span class=md-ellipsis> Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=../agent-build/setup/ class=md-nav__link> <span class=md-ellipsis> Installation Guide </span> </a> </li> <li class=md-nav__item> <a href=../agent-build/ class=md-nav__link> <span class=md-ellipsis> Agent Build </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class=md-ellipsis> Support </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Support </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../agent-build/developer/ class=md-nav__link> <span class=md-ellipsis> Developer Guide </span> </a> </li> <li class=md-nav__item> <a href=../agent-build/whatsapp/ class=md-nav__link> <span class=md-ellipsis> Integration </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 id=__nav_3_label tabindex> <span class=md-ellipsis> Agent Test </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=true> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Agent Test </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=setup/ class=md-nav__link> <span class=md-ellipsis> Installation Guide </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> Agent Test </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Agent Test </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#revolutionizing-how-teams-test-evaluate-and-perfect-their-conversational-ai class=md-nav__link> <span class=md-ellipsis> REVOLUTIONIZING HOW TEAMS TEST, EVALUATE, AND PERFECT THEIR CONVERSATIONAL AI </span> </a> </li> <li class=md-nav__item> <a href=#the-architecture-behind-the-roles class=md-nav__link> <span class=md-ellipsis> The Architecture Behind the Roles </span> </a> </li> <li class=md-nav__item> <a href=#prompt-management-with-langfuse class=md-nav__link> <span class=md-ellipsis> Prompt Management with Langfuse </span> </a> <nav class=md-nav aria-label="Prompt Management with Langfuse"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#agent_testpy-test-case-generation-prompt-validation class=md-nav__link> <span class=md-ellipsis> /agent_test.py – Test Case Generation &amp; Prompt Validation </span> </a> <nav class=md-nav aria-label="/agent_test.py – Test Case Generation & Prompt Validation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#code-snippets class=md-nav__link> <span class=md-ellipsis> Code snippets: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#agent_evalpy-llmdriven-evaluation-scoring class=md-nav__link> <span class=md-ellipsis> /agent_eval.py – LLM‑Driven Evaluation &amp; Scoring </span> </a> <nav class=md-nav aria-label="/agent_eval.py – LLM‑Driven Evaluation & Scoring"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#code-snippets_1 class=md-nav__link> <span class=md-ellipsis> Code snippets: </span> </a> <nav class=md-nav aria-label="Code snippets:"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#inside-the-logic class=md-nav__link> <span class=md-ellipsis> Inside the logic: </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#heres-how-you-can-start-testing-with-agenttest-ready class=md-nav__link> <span class=md-ellipsis> Here’s how you can start testing with AgentTest — READY? </span> </a> </li> <li class=md-nav__item> <a href=#what-youll-see class=md-nav__link> <span class=md-ellipsis> What You’ll See </span> </a> </li> <li class=md-nav__item> <a href=#passing-parameters class=md-nav__link> <span class=md-ellipsis> Passing Parameters </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#revolutionizing-how-teams-test-evaluate-and-perfect-their-conversational-ai class=md-nav__link> <span class=md-ellipsis> REVOLUTIONIZING HOW TEAMS TEST, EVALUATE, AND PERFECT THEIR CONVERSATIONAL AI </span> </a> </li> <li class=md-nav__item> <a href=#the-architecture-behind-the-roles class=md-nav__link> <span class=md-ellipsis> The Architecture Behind the Roles </span> </a> </li> <li class=md-nav__item> <a href=#prompt-management-with-langfuse class=md-nav__link> <span class=md-ellipsis> Prompt Management with Langfuse </span> </a> <nav class=md-nav aria-label="Prompt Management with Langfuse"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#agent_testpy-test-case-generation-prompt-validation class=md-nav__link> <span class=md-ellipsis> /agent_test.py – Test Case Generation &amp; Prompt Validation </span> </a> <nav class=md-nav aria-label="/agent_test.py – Test Case Generation & Prompt Validation"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#code-snippets class=md-nav__link> <span class=md-ellipsis> Code snippets: </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#agent_evalpy-llmdriven-evaluation-scoring class=md-nav__link> <span class=md-ellipsis> /agent_eval.py – LLM‑Driven Evaluation &amp; Scoring </span> </a> <nav class=md-nav aria-label="/agent_eval.py – LLM‑Driven Evaluation & Scoring"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#code-snippets_1 class=md-nav__link> <span class=md-ellipsis> Code snippets: </span> </a> <nav class=md-nav aria-label="Code snippets:"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#inside-the-logic class=md-nav__link> <span class=md-ellipsis> Inside the logic: </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#heres-how-you-can-start-testing-with-agenttest-ready class=md-nav__link> <span class=md-ellipsis> Here’s how you can start testing with AgentTest — READY? </span> </a> </li> <li class=md-nav__item> <a href=#what-youll-see class=md-nav__link> <span class=md-ellipsis> What You’ll See </span> </a> </li> <li class=md-nav__item> <a href=#passing-parameters class=md-nav__link> <span class=md-ellipsis> Passing Parameters </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 class=title id=agenttest-from-prompt-to-performance>AgentTest: From Prompt To Performance</h1> <h4 class=sub-title id=revolutionizing-how-teams-test-evaluate-and-perfect-their-conversational-ai>REVOLUTIONIZING HOW TEAMS TEST, EVALUATE, AND PERFECT THEIR CONVERSATIONAL AI</h4> <p>Quality assurance for conversational AI agents presents critical challenges: manual test creation scales poorly, evaluation criteria lack consistency, and comprehensive edge case coverage remains difficult to achieve systematically. Development teams face hurdles in validating agent behavior across diverse scenarios while maintaining objective assessment standards.</p> <p>Imagine handing your AI agent to an intelligent testing system that operates autonomously — generating its own test cases, judging responses with expert precision, and delivering clear, actionable insights.</p> <p>That's exactly what AgentTest delivers.</p> <p>One prompt is all it takes — <strong> AgentTest </strong>handles the rest</p> <div style="display: flex; justify-content: space-around; text-align: center; margin-top: 2rem; margin-bottom: 2rem; gap: 2rem; flex-wrap: wrap;"> <div style="flex: 1; min-width: 200px;"> <img src=/images/creator.png alt="Creator Icon" style="width: 100px; height: 100px;"> <p><strong>Creator</strong><br> <em>“Generating</em> varied<br> and high impact<br> test cases–automatically”</p> </div> <div style="flex: 1; min-width: 200px;"> <img src=/images/judge.png alt="Judge Icon" style="width: 100px; height: 100px;"> <p><strong>Judge</strong><br> <em>“Evaluating</em> every response<br> across multiple performance<br> metrics – with precision”</p> </div> <div style="flex: 1; min-width: 200px;"> <img src=/images/analyst.png alt="Analyst Icon" style="width: 100px; height: 100px;"> <p><strong>Analyst</strong><br> <em>“Presenting</em> results<br> as a clear, interactive<br> report you can act on”</p> </div> </div> <p><strong>Onward, AgentTest expands on these roles with a fully automated testing pipeline</strong> — taking your agent’s prompt and transforming it into a comprehensive suite of test cases, spanning everyday requests to edge‑case scenarios in multiple languages. By structuring all outputs in JSON, it ensures compatibility with automated evaluation, debugging, and continuous improvement workflows.</p> <p><strong>Beyond generating test cases, AgentTest also serves as a judge for your AI agents.</strong> Each response from the agent is evaluated using a Langfuse‑driven scoring rubric, which measures performance across key dimensions such as language quality, requirement coverage, tone appropriateness, domain relevance, factual accuracy, and task fulfillment. This structured evaluation transforms subjective assessments into quantifiable, reproducible results, allowing teams to pinpoint strengths and areas for improvement. </p> <p><strong>The framework further evolves into an analyst, distilling test outcomes into an interactive, insight‑driven dashboard.</strong> It displays test results via visual reports, pass/fail summaries, and detailed feedback for each case. Whether you're validating changes or tracking regressions, AgentTest provides the clarity and confidence you need to iterate fast and improve effectively. </p> <p><strong>“Together, these capabilities position AgentTest as a unified, end‑to‑end testing pipeline for seamless agent validation”</strong></p> <h2 class=sub-heading id=the-architecture-behind-the-roles>The Architecture Behind the Roles</h2> <p><img alt class=projected-img src=images/test1.png></p> <p>Thus far, we’ve explored the conceptual architecture — now let’s map it to the actual Python modules, going beyond high‑level concepts into implementation.</p> <h2 class=sub-heading id=prompt-management-with-langfuse>Prompt Management with Langfuse</h2> <p><strong><a href=https://langfuse.com/docs/prompt-management/get-started>LangfusePromptManagement</a></strong> helps to version control and manage prompts collaboratively in one place. It’s where teams (both technical and otherwise) create, organize, and iterate on prompts without digging into the codebase.</p> <p>The prompt management dashboard lists all prompts by name and version. From there we can create, edit, label, and version-control prompts directly through the web console.</p> <p>In our project, we integrated Langfuse to fetch prompts dynamically at runtime using a dedicated LangfusePromptManager class (defined in langfuse_prompt_manager.py). This class initializes a Langfuse client using secure environment variables and provides a simple method to retrieve prompts by name.</p> <p>By using this setup, prompt logic remains completely decoupled from application logic, making it easy to iterate on prompts during development and testing — without needing to redeploy the system. Any changes made on the Langfuse dashboard are reflected instantly in the agent behavior during test generation or evaluation.</p> <p>Here’s a simplified view of how the prompt is retrieved in our code: <div class=highlight><pre><span></span><code><span class=c1># Fetching a prompt from Langfuse</span>
<span class=n>prompt_manager</span> <span class=o>=</span> <span class=n>LangfusePromptManager</span><span class=p>()</span>
<span class=n>prompt</span> <span class=o>=</span> <span class=n>prompt_manager</span><span class=o>.</span><span class=n>get_prompt</span><span class=p>(</span><span class=s2>&quot;your_prompt_name&quot;</span><span class=p>)</span>
</code></pre></div> This integration has significantly improved flexibility and collaboration during prompt tuning, while maintaining a clean separation of responsibilities within the codebase.</p> <h4 id=agent_testpy-test-case-generation-prompt-validation><code>/agent_test.py</code> – Test Case Generation &amp; Prompt Validation</h4> <p>As introduced earlier, This module handles the automated generation of test cases — implemented in agent_test.py. It begins by fetching a reusable test case template from Langfuse through LangfusePromptManager, dynamically fills it with the agent’s configuration prompt and desired number of cases, and then invokes an LLM to generate structured input–output examples.</p> <h5 id=code-snippets>Code snippets:</h5> <p>Template retrieval</p> <p>The generator retrieves a Langfuse-stored prompt template using LangfusePromptManager: <div class=highlight><pre><span></span><code><span class=n>template</span> <span class=o>=</span> <span class=n>prompt_manager</span><span class=o>.</span><span class=n>get_prompt</span><span class=p>(</span><span class=s2>&quot;agent_test&quot;</span><span class=p>)</span>
</code></pre></div> Template filling &amp; llm invocation</p> <p>The agent prompt and test case count are dynamically inserted into the template: <div class=highlight><pre><span></span><code><span class=n>filled_prompt</span> <span class=o>=</span> <span class=n>fill_prompt</span><span class=p>(</span><span class=n>template</span><span class=p>,</span> <span class=p>{</span>
    <span class=s2>&quot;agent_prompt&quot;</span><span class=p>:</span> <span class=n>agent_prompt</span><span class=p>,</span>
    <span class=s2>&quot;num_cases&quot;</span><span class=p>:</span> <span class=n>n</span><span class=p>,</span>
<span class=p>})</span>
<span class=n>llm_output</span> <span class=o>=</span> <span class=n>call_llm</span><span class=p>(</span><span class=n>filled_prompt</span><span class=p>)</span>
</code></pre></div> Robust json extraction</p> <p>LLM outputs are often noisy or malformed. To handle this, the module implements a recovery parser: <div class=highlight><pre><span></span><code><span class=k>def</span><span class=w> </span><span class=nf>extract_json</span><span class=p>(</span><span class=n>text</span><span class=p>:</span> <span class=nb>str</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>:</span>
    <span class=c1># Attempts clean JSON parsing</span>
    <span class=c1># Falls back to partial extraction of valid objects</span>
</code></pre></div></p> <blockquote> <p>“To maintain test diversity and prevent redundancy, the module derives a unique signature for each generated case and filters out duplicates before saving. This ensures that every test case contributes new coverage rather than repeating existing ones, making the generated suite more comprehensive and realistic for agent evaluation.”</p> </blockquote> <h4 id=agent_evalpy-llmdriven-evaluation-scoring><code>/agent_eval.py</code> – LLM‑Driven Evaluation &amp; Scoring</h4> <p>This module acts as an autonomous evaluation engine, using a Langfuse‑managed evaluation template and an LLM‑based judge to score agent responses against user expectations. It evaluates multiple dimensions — language quality, requirement coverage, tone, domain consistency, factual accuracy, and task fulfillment — producing a structured, explainable JSON report for each test case.</p> <h5 id=code-snippets_1>Code snippets:</h5> <p>Evaluation template retrieval</p> <p>An evaluation prompt is fetched from Langfuse to ensure consistent, centrally managed criteria: <div class=highlight><pre><span></span><code><span class=n>template</span> <span class=o>=</span> <span class=n>prompt_manager</span><span class=o>.</span><span class=n>get_prompt</span><span class=p>(</span><span class=s2>&quot;agent_eval&quot;</span><span class=p>)</span>
</code></pre></div></p> <p>Dynamic context injection</p> <p>The test case’s user input and the agent’s response are injected into the template before sending to the LLM: <div class=highlight><pre><span></span><code><span class=n>filled_prompt</span> <span class=o>=</span> <span class=n>fill_prompt_template</span><span class=p>(</span><span class=n>template</span><span class=p>,</span> <span class=p>{</span>
    <span class=s2>&quot;user_input&quot;</span><span class=p>:</span> <span class=n>user_input</span><span class=p>,</span>
    <span class=s2>&quot;llm_response&quot;</span><span class=p>:</span> <span class=n>llm_response</span>
<span class=p>})</span>
</code></pre></div></p> <p>LLM‑based judgement &amp; json parsing</p> <p>The LLM produces a structured evaluation, which is then cleaned and parsed: <div class=highlight><pre><span></span><code><span class=n>raw_output</span> <span class=o>=</span> <span class=n>call_llm</span><span class=p>(</span><span class=n>filled_prompt</span><span class=p>)</span>
<span class=n>cleaned_output</span> <span class=o>=</span> <span class=n>strip_markdown_json</span><span class=p>(</span><span class=n>raw_output</span><span class=p>)</span>
<span class=n>evaluation</span> <span class=o>=</span> <span class=n>json</span><span class=o>.</span><span class=n>loads</span><span class=p>(</span><span class=n>cleaned_output</span><span class=p>)</span>
</code></pre></div></p> <blockquote> <h6 id=inside-the-logic><strong>Inside the logic:</strong></h6> <ul> <li>Date‑aware reasoning: Interprets relative dates (e.g., “tomorrow”, “next weekend”) based on the current date ({{today}}) and penalizes incorrect interpretations.</li> <li>Hallucination detection: It flags responses where the agent fabricates details instead of requesting clarification.</li> <li>Error‑tolerant parsing: If the LLM returns malformed JSON, the module logs the raw response and gracefully recovers, ensuring no evaluation is silently lost.</li> <li>Explainable scoring: Each dimension includes a score and justification, making the evaluation transparent and actionable for developers.</li> </ul> </blockquote> <h4 id=heres-how-you-can-start-testing-with-agenttest-ready>Here’s how you can start testing with AgentTest — READY?</h4> <p>Follow these steps :</p> <ol> <li> <p>Upload your agent configuration prompt.</p> </li> <li> <p>Confirm its validity </p> </li> </ol> <div class=highlight><pre><span></span><code>Given a prompt is submitted for evaluation 
When the system checks the prompt 
Then the prompt must satisfy the following: 
<span class=k>-</span><span class=w> </span>The prompt must include enough context or rules for response generation 
<span class=k>-</span><span class=w> </span>The length of the prompt must be at least 100 characters 
<span class=k>-</span><span class=w> </span>The prompt must be written in a supported and properly formatted language 
</code></pre></div> <div class="admonition note"> <p class=admonition-title>Note</p> <p>If the prompt does not meet these conditions, the prompt will be rejected.</p> </div> <h4 id=what-youll-see>What You’ll See</h4> <p>Once the evaluation begins, AgentTest provides,</p> <ul> <li>Visual metrics dashboard showing pass/fail breakdown.</li> <li>Individual test case insights — showing what failed and why.</li> <li>Downloadable JSON reports: For deeper inspection or integration into QA pipelines.</li> </ul> <h4 id=passing-parameters>Passing Parameters</h4> <p>A test is considered passed when,</p> <ul> <li>All metric scores are consistently high (typically ≥4/5).</li> <li>The Judge’s final verdict is "overall_judgment": "Pass"</li> </ul> <p><div class=highlight><pre><span></span><code>Given<span class=w> </span>an<span class=w> </span>agent<span class=w> </span>response<span class=w> </span>is<span class=w> </span>evaluated
When<span class=w> </span>the<span class=w> </span>Judge<span class=w> </span>LLM<span class=w> </span>analyzes<span class=w> </span>the<span class=w> </span>output
Then<span class=w> </span>it<span class=w> </span>is<span class=w> </span>scored<span class=w> </span>based<span class=w> </span>on:
<span class=w>  </span>-<span class=w> </span>Language<span class=w> </span>Quality:<span class=w> </span>checks<span class=w> </span>grammar<span class=w> </span>and<span class=w> </span>fluency
<span class=w>  </span>-<span class=w> </span>Requirement<span class=w> </span>Coverage:<span class=w> </span>ensures<span class=w> </span>all<span class=w> </span>input<span class=w> </span>details<span class=w> </span>are<span class=w> </span>addressed
<span class=w>  </span>-<span class=w> </span>Tone<span class=w> </span>Appropriateness:<span class=w> </span>assesses<span class=w> </span>politeness<span class=w> </span>and<span class=w> </span>helpfulness
<span class=w>  </span>-<span class=w> </span>Domain<span class=w> </span>Appropriateness:<span class=w> </span>confirms<span class=w> </span>relevance<span class=w> </span>to<span class=w> </span>agent’s<span class=w> </span>role
<span class=w>  </span>-<span class=w> </span>Factual<span class=w> </span>Accuracy:<span class=w> </span>flags<span class=w> </span>hallucinated<span class=w> </span>or<span class=w> </span>incorrect<span class=w> </span>content
<span class=w>  </span>-<span class=w> </span>Task<span class=w> </span>Fulfillment:<span class=w> </span>evaluates<span class=w> </span>whether<span class=w> </span>the<span class=w> </span>user’s<span class=w> </span>goal<span class=w> </span>is<span class=w> </span>achieved
</code></pre></div> Want to see it in action? Watch the video </p> <div style="display: flex; justify-content: center; margin-top: 1rem;"> <video width=720 height=auto controls preload=metadata style="box-shadow: 0 4px 12px rgba(0,0,0,0.15); border-radius: 8px;"> <source src=/assets/demo.mp4 type=video/mp4> </video> </div> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["navigation.tabs", "navigation.sections"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.50899def.min.js></script> </body> </html>